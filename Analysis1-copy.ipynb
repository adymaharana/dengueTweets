{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import Python's json Encoder and Decoder\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "#with open('dengueDump_3.23.16_validated.json') as json_data:\n",
    "#    d = json.load(json_data)\n",
    "#    json_data.close()\n",
    "#    print(d)\n",
    "\n",
    "# Parsing of json file\n",
    "    \n",
    "data = []\n",
    "with open('dengueDump_3.23.16_validated.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13513\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "#pprint(data[1]['lang'])\n",
    "# pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10116"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraction of portuguese tweets\n",
    "\n",
    "pt_tweets = []\n",
    "for tweet in data:\n",
    "    # Portuguese Tweets are encoded as 'pt' and spanish tweets are encoded as 'es'\n",
    "    if tweet['lang'] == 'pt':\n",
    "        pt_tweets.append(tweet)\n",
    "\n",
    "len(pt_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9965"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraction of tweets tagged as junk(1) or sickness(3) only\n",
    "\n",
    "pt13_tweets = []\n",
    "for tweet in pt_tweets:\n",
    "    # Identify curator ids of each tweet to refer their tags\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for i in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][i]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "#    cid1 = tweet['edits'][0]['curator_id']\n",
    "#    cid2 = tweet['edits'][1]['curator_id']\n",
    "    if (tweet['tags'][cid1] != 2 and tweet['tags'][cid2] != 2):\n",
    "        pt13_tweets.append(tweet)\n",
    "\n",
    "len(pt13_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7758\n",
      "2207\n"
     ]
    }
   ],
   "source": [
    "# Differentiating tweets into those with common tags and those with different tags - easier for manipulation\n",
    "\n",
    "cmntags = []\n",
    "difftags = []\n",
    "zerotag = []\n",
    "for tweet in pt13_tweets:\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for i in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][i]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        cmntags.append(tweet)\n",
    "    else:\n",
    "        difftags.append(tweet)\n",
    "    \n",
    "    if ((tweet['tags'][cid1] == 0)|(tweet['tags'][cid2] == 0)):\n",
    "        zerotag.append(tweet)\n",
    "\n",
    "print(len(cmntags))\n",
    "print(len(difftags))\n",
    "# pprint(zerotag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497\n"
     ]
    }
   ],
   "source": [
    "count3 = 0\n",
    "for tweet in cmntags:\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    if(tweet['tags'][cid] == 3):\n",
    "        count3 = count3 + 1\n",
    "# end\n",
    "print(count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "rec = openpyxl.load_workbook('Tweets_reverse_geolocated_updated.xlsx')\n",
    "rec.get_sheet_names()\n",
    "sheet = rec.get_sheet_by_name('Tweets_reverse_geolocated_updat')\n",
    "\n",
    "region = []\n",
    "state = []\n",
    "county = []\n",
    "mregion = []\n",
    "for i in range(2, 28126):\n",
    "    val = [sheet.cell(row = i, column = 7).value]\n",
    "    region.extend(val)\n",
    "    val = [sheet.cell(row = i, column = 8).value]\n",
    "    state.extend(val)\n",
    "    val = [sheet.cell(row = i, column = 9).value]\n",
    "    county.extend(val)\n",
    "    val = [sheet.cell(row = i, column = 10).value]\n",
    "    mregion.extend(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n",
      "205\n",
      "2573\n",
      "462\n"
     ]
    }
   ],
   "source": [
    "regionSet = set(region)\n",
    "stateSet = set(state)\n",
    "countySet = set(county)\n",
    "mregionSet = set(mregion)\n",
    "\n",
    "print(len(regionSet))\n",
    "print(len(stateSet))\n",
    "print(len(countySet))\n",
    "print(len(mregionSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetID,userID,date,lat,lon\n",
      "\n",
      "408779568778997760,354120599,2013-12-05 21:06:58,25.746,-100.277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfile = open('dengueDump_3.23.16_coordinates.txt', 'r')\n",
    "print(cfile.readline())\n",
    "print(cfile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import regex\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Define a function for pre-processing of tweets\n",
    "def processTweet(tweet):\n",
    "    \n",
    "    # t = \"blah #ToKi9llAMockingbird #onemore blah\"\n",
    "    # t1 = re.match(r'#([^\\s])', t)\n",
    "    # t = \"#A\"\n",
    "    #Convert to lower case\n",
    "#     tweet = tweet.lower()\n",
    "    #Convert hyperlinks to a generic term 'URL'\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?:\\/\\/[^\\s]+))','',tweet)\n",
    "    #Convert @username to USER\n",
    "    tweet = re.sub('(@[^\\s]+)|(@[\\s][^\\s]+)','',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "#     tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end\n",
    "\n",
    "# Generate a text file of the original tweets and processed tweets for inspection \n",
    "# fp = open('compareTweets.txt', 'w')\n",
    "# fp1 = open('processedTweets.txt', 'w')\n",
    "\n",
    "# for tweet in cmntags:\n",
    "#     text = tweet['t']\n",
    "#     processedtext = processTweet(text)\n",
    "#     fp.write(text)\n",
    "#     fp.write('\\t')\n",
    "#     fp.write(processedtext)\n",
    "#     fp1.write(processedtext)\n",
    "#     fp.write('\\n')\n",
    "#     fp1.write('\\n')\n",
    "# #end loop\n",
    "# fp.close()\n",
    "# fp1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "# print(stopwords[0:10])\n",
    "stopwords.append('USER')\n",
    "stopwords.append('URL')\n",
    "\n",
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.*)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "#start getfeatureVector\n",
    "def getFeatureVector(tweet, featureVector):\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = re.sub(r'#([^\\s]+)', r'\\1', w)\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,*._')\n",
    "        \n",
    "#         check if the word starts with an alphabet\n",
    "#         val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        val = re.search(r\"^[a-zA-Z]\", w)\n",
    "\n",
    "#         check if the word contains only numbers\n",
    "#         valnum = re.search(r\"^[0-9][0-9]*$\", w)\n",
    "    \n",
    "        # Experiment\n",
    "        u = w.encode('unicode-escape')\n",
    "        bval = re.search(b'\\\\U', u)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stopwords or val is None):\n",
    "#         if(w in stopwords or valnum):\n",
    "            if (bval is None):\n",
    "                continue\n",
    "            else:\n",
    "                featureVector.append(w)\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "        regex = re.compile(r'#([^\\s]+)')\n",
    "        \n",
    "    regex = re.compile(r'#([^\\s]+)')\n",
    "    matchObj = regex.findall(tweet)\n",
    "#     print(matchObj)\n",
    "    s = len(matchObj)\n",
    "    fv = []\n",
    "    for i in range(0,s):\n",
    "        word = matchObj[i]\n",
    "        startInd = len(word)\n",
    "        stopInd = 0\n",
    "        for i in range(0,len(word)):\n",
    "            if (i==(len(word)-1)):\n",
    "                        stopInd = i + 1\n",
    "                        if (startInd == len(word)):\n",
    "                            startInd = 0\n",
    "                        fv.append(word[startInd:stopInd].lower())\n",
    "                        continue\n",
    "            if (word[i].isupper()):\n",
    "                if (startInd != len(word)):\n",
    "                    stopInd = i\n",
    "#                     print(i)\n",
    "#                     print(word[startInd:stopInd])\n",
    "                    fv.append(word[startInd:stopInd].lower())\n",
    "                    startInd = i\n",
    "                else:\n",
    "                    if (i != 0):\n",
    "                        startInd = 0\n",
    "                        stopInd = i\n",
    "                        fv.append(word[startInd:stopInd].lower())\n",
    "                        startInd = i\n",
    "    \n",
    "    featureVector.extend(fv)\n",
    "    return featureVector\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "3103.2\n",
      "0\n",
      "[]\n",
      "[[9]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i)\n",
    "\n",
    "print(round(1.49))\n",
    "dsize = len(cmntags)\n",
    "print(2*dsize/5)\n",
    "d = [1, 9, 0, 5]\n",
    "t = d[:0]\n",
    "print(len(t))\n",
    "print(t)\n",
    "t.append(d[1:2])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# featureList = []\n",
    "# tweetset = []\n",
    "# trainset = cmntags[0:5999]\n",
    "# for tweet in trainset:\n",
    "#     tweetFV = []\n",
    "#     text = tweet['t']\n",
    "#     cid = tweet['edits'][0]['curator_id']\n",
    "#     tag = tweet['tags'][cid]\n",
    "#     processedtext = processTweet(text)\n",
    "#     tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "#     featureList.extend(tweetFV)\n",
    "#     tweetset.append((tweetFV,tag))\n",
    "\n",
    "# print(len(featureList))\n",
    "# featureListSet = set(featureList)\n",
    "# print(len(featureListSet))\n",
    "# featureList = list(featureListSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14611"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparation of test data\n",
    "data = []\n",
    "with open('dengueDump_3.23.16_nonvalidated.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '452666295931371520',\n",
      " 'acr': '2011-05-29T23:48:51',\n",
      " 'cc': 'MX',\n",
      " 'cr': '2014-04-06T00:37:09',\n",
      " 'f': 'tw20144517192',\n",
      " 'flng': 123,\n",
      " 'flrs': 174,\n",
      " 'lang': 'es',\n",
      " 'loc': 'Mexico',\n",
      " 'p': '7d93122509633720',\n",
      " 'pln': -99.161,\n",
      " 'plt': 19.381,\n",
      " 't': 'Cumple dengue (@ El Viejo Camilo w/ @carro_1986) '\n",
      "      'http://t.co/jyD9U6w5uD',\n",
      " 'tln': -99.155,\n",
      " 'tlt': 19.386,\n",
      " 'uid': '307695404'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10622"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_tweets = []\n",
    "for tweet in data:\n",
    "    # Portuguese Tweets are encoded as 'pt' and spanish tweets are encoded as 'es'\n",
    "    if tweet['lang'] == 'pt':\n",
    "        pt_tweets.append(tweet)\n",
    "\n",
    "len(pt_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start extract_features\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "#end\n",
    "\n",
    "# Extract feature vector for all tweets in one shote\n",
    "# training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "# Train the classifier\n",
    "# NBClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fp = open('outputNB2.txt', 'w')\n",
    "# testset = cmntags[6000:]\n",
    "# acc = 0\n",
    "# for tweet in testset:\n",
    "#     fv = []\n",
    "#     # Test the classifier\n",
    "#     testTweet = tweet['t']\n",
    "#     processedTestTweet = processTweet(testTweet)\n",
    "#     x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "#     fp.write(testTweet)\n",
    "#     fp.write('\\t')\n",
    "#     fp.write(str(x))\n",
    "#     fp.write('\\t')\n",
    "#     cid = tweet['edits'][0]['curator_id']\n",
    "#     fp.write(str(tweet['tags'][cid]))\n",
    "#     fp.write('\\n')\n",
    "#     if (x == tweet['tags'][cid]):\n",
    "#         acc = acc + 1\n",
    "\n",
    "# fp.close()\n",
    "# print(acc)\n",
    "# acc = acc/1858\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "rec = openpyxl.load_workbook('Tweets_reverse_geolocated_updated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tweetID'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.get_sheet_names()\n",
    "sheet = rec.get_sheet_by_name('Tweets_reverse_geolocated_updat')\n",
    "sheet['A1'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "region = []\n",
    "state = []\n",
    "county = []\n",
    "mregion = []\n",
    "for i in range(2, 28126):\n",
    "    val = [sheet.cell(row = i, column = 7).value]\n",
    "    region.extend(val)\n",
    "    val = [sheet.cell(row = i, column = 8).value]\n",
    "    state.extend(val)\n",
    "    val = [sheet.cell(row = i, column = 9).value]\n",
    "    county.extend(val)\n",
    "    val = [sheet.cell(row = i, column = 10).value]\n",
    "    mregion.extend(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regionSet = set(region)\n",
    "stateSet = set(state)\n",
    "countySet = set(county)\n",
    "mregionSet = set(mregion)\n",
    "\n",
    "# print(len(regionSet))\n",
    "# print(len(stateSet))\n",
    "# print(len(countySet))\n",
    "# print(len(mregionSet))\n",
    "\n",
    "# print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweetID', 'userID', 'date', 'lat', 'lon\\n']\n"
     ]
    }
   ],
   "source": [
    "tweetID = []\n",
    "cfile = open('dengueDump_3.23.16_coordinates.txt', 'r')\n",
    "x = (cfile.readline()).split(',')\n",
    "print(x)\n",
    "for i in range(0,28126):\n",
    "    x = (cfile.readline()).split(',')\n",
    "    tweetID.extend(x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training & n-fold Cross Validation of Naive Bayes classifier\n",
    "\n",
    "count = 0\n",
    "n = 5\n",
    "dsize = len(cmntags)\n",
    "finacc = 0\n",
    "finprec = 0\n",
    "fintnr = 0\n",
    "finrecall = 0\n",
    "flist = open('featureList5.txt','w')\n",
    "fvtweet = open('fvTweet','w')\n",
    "\n",
    "for i in range(0,1):\n",
    "    testset = pt_tweets\n",
    "    trainset = cmntags\n",
    "#     ind1 = int(i*dsize/n)\n",
    "#     ind2 = int((i+1)*dsize/n)\n",
    "#     testset = cmntags[ind1:ind2]\n",
    "#     trainset = cmntags[:ind1]\n",
    "#     trainset.extend(cmntags[ind2:])\n",
    "    \n",
    "    featureList = []\n",
    "    tweetset = []\n",
    "    \n",
    "    #Exp\n",
    "    tagList = []\n",
    "    \n",
    "#    trainset = cmntags[0:5999]\n",
    "    for tweet in trainset:\n",
    "        tweetFV = []\n",
    "        text = tweet['t']\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        tag = tweet['tags'][cid]\n",
    "        \n",
    "        # Experiment\n",
    "        tagList.append(tag)\n",
    "        \n",
    "        processedtext = processTweet(text)\n",
    "        tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "        featureList.extend(tweetFV)\n",
    "        for i in range(0,len(tweetID)):\n",
    "            if (tweetID[i] == tweet['_id']):\n",
    "                tweetFV.extend(region[i])\n",
    "                tweetFV.extend(state[i])\n",
    "                tweetFV.extend(county[i])\n",
    "                tweetFV.extend(mregion[i])\n",
    "                break\n",
    "        \n",
    "        tweetset.append((tweetFV,tag))\n",
    "\n",
    "    featureList.extend(list(regionSet))\n",
    "    featureList.extend(list(stateSet))\n",
    "    featureList.extend(list(countySet))\n",
    "    featureList.extend(list(mregionSet))\n",
    "    featureList.extend(list(stateSet))\n",
    "    print(len(featureList))\n",
    "    featureListSet = set(featureList)\n",
    "    print(len(featureListSet))\n",
    "    featureList = list(featureListSet)\n",
    "#     flist.write(\"\\n\".join(featureList))\n",
    "    \n",
    "    tagListSet = set(tagList)\n",
    "    tagList = list(tagListSet)\n",
    "    print(tagList)\n",
    "    print('FVs done')\n",
    "\n",
    "    # Extract feature vector for all tweets in one shote\n",
    "    training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "    # Train the classifier\n",
    "    NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    \n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    tn = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(0,len(testset)):\n",
    "    tweet = testset[j]\n",
    "    fv = []\n",
    "    # Test the classifier\n",
    "    testTweet = tweet['t']\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    if (x == 3):\n",
    "        count = count + 1\n",
    "    pt_tweets[j]['ctags'] = x\n",
    "#         fp.write(testTweet)\n",
    "#         fp.write('\\t')\n",
    "#         fp.write(str(x))\n",
    "#         fp.write('\\t')\n",
    "#         cid = tweet['edits'][0]['curator_id']\n",
    "#         fp.write(str(tweet['tags'][cid]))\n",
    "#         fp.write('\\n')\n",
    "#         if (x == tweet['tags'][cid]):\n",
    "#             if (x == 1):\n",
    "#                 tn = tn + 1\n",
    "#             else:\n",
    "#                 tp = tp + 1\n",
    "#         else:\n",
    "#             if (x == 1):\n",
    "#                 fn = fn + 1\n",
    "#             else:\n",
    "#                 fp = fp + 1\n",
    "\n",
    "# fp.close()\n",
    "#     acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "#     prec = tp/(tp + fp)\n",
    "#     recall = tp/(tp + fn)\n",
    "#     tnr = tn/(tn + fp)\n",
    "#     print('Accuracy')\n",
    "#     print(acc)\n",
    "#     print('Precision')\n",
    "#     print(prec)\n",
    "#     print('Recall')\n",
    "#     print(recall)\n",
    "#     print('TNR')\n",
    "#     print(tnr)\n",
    "#     finacc = finacc + acc\n",
    "#     finrecall = finrecall + recall\n",
    "#     finprec = finprec + prec\n",
    "#     fintnr = fintnr + tnr\n",
    "\n",
    "# finacc = finacc/n\n",
    "# finrecall = finrecall/n\n",
    "# finprec = finprec/n\n",
    "# fintnr = fintnr/n\n",
    "# print(finacc)\n",
    "# print(finprec)\n",
    "# print(finrecall)\n",
    "# print(fintnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10622\n",
      "4074\n"
     ]
    }
   ],
   "source": [
    "print(len(pt_tweets))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = []\n",
    "with open('tweet_master_data2.json') as f:\n",
    "    for line in f:\n",
    "        data2.extend(json.loads(line))\n",
    "\n",
    "import csv\n",
    "fp = open('test5.csv', 'w', newline='')\n",
    "a = csv.writer(fp, delimiter=',')\n",
    "a.writerow(('Time Stamp', 'Tweet Longitude', 'Tweet Latitude', 'Country', 'Region', 'State', 'County', 'Microregion'))\n",
    "\n",
    "for k in range(0,len(pt_tweets)):\n",
    "    tweet = pt_tweets[k]\n",
    "#     if (tweet['lang'] == 'es')|(tweet['country'] != 'Brasil'):\n",
    "#         continue\n",
    "#     editlen = len(tweet['edits'])\n",
    "#     cidList = []\n",
    "#     for j in range(0,editlen):\n",
    "#         cidList.append(tweet['edits'][j]['curator_id'])\n",
    "#     cidListSet = set(cidList)\n",
    "#     cidList = list(cidListSet)\n",
    "#     cid1 = cidList[0]\n",
    "#     cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "#     if (tweet['ctags'] == 1):\n",
    "#         continue\n",
    "#         print('Common Tags')\n",
    "#         if (tweet['tags'][cid1] == 3):\n",
    "#             print('Sickness Tweet')\n",
    "#             a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))\n",
    "#     else:\n",
    "#         print('Different Tags')\n",
    "    testTweet = tweet['t']\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    data2[k].update({'ctags':x})\n",
    "#         print('Classified')\n",
    "    if (tweet['ctags'] == 3):\n",
    "        for m in range(0,len(data2)):\n",
    "            tweet2 = data2[m]\n",
    "            if (tweet['_id'] == tweet2['_id']):\n",
    "                if(tweet2['country'] != 'Brasil'):\n",
    "                    break\n",
    "                else:\n",
    "                    a.writerow((tweet2['cr'], tweet2['tln'], tweet2['tlt'], tweet2['country'], tweet2['region'], tweet2['state'], tweet2['county'], tweet2['microregion']))\n",
    "                    break\n",
    "            \n",
    "\n",
    "# end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"I\\like\\my!friends\"\n",
    "print(s.split('!'))\n",
    "print(featureList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "line = 'Cats #AsYouLikeIt fine'\n",
    "\n",
    "matchObj = re.match(r'#([^\\s]+)', line, re.I)\n",
    "\n",
    "if matchObj:\n",
    "    print(matchObj.group())\n",
    "else:\n",
    "    print('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ss rksa pod\n"
     ]
    }
   ],
   "source": [
    "tweet = \"Ss #rksa pod\"\n",
    "tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "print(tweet)\n",
    "\n",
    "f = open(\"Results-1-FV.txt\",\"w\")\n",
    "# print(featureList, file=f)\n",
    "f.writelines([\"%s\\t\" % item  for item in featureList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print informative features about the classifier\n",
    "print(NBClassifier.show_most_informative_features(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "indList = []\n",
    "for i in range(0,dsize):\n",
    "        tweet = cmntags[i]\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        tag = tweet['tags'][cid]\n",
    "        if (tag == 0):\n",
    "            indList.append(i)\n",
    "\n",
    "print(len(indList))\n",
    "print(indList)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '466148566793465856', 'pln': -46.423, 'tags': {'540e300a7673826b17a5604c': 1, '555232868624c82a1c6d2ca3': 1}, 'loc': 'Santo André-SP', 'p': '35e1542602b65f19', 'tln': -46.54, 'flng': 2002, 'lang': 'pt', 'v': True, 'plt': -23.711, 'cr': '2014-05-13T05:30:53', 'tlt': -23.649, 'flrs': 686, 'acr': '2009-07-16T06:21:48', 't': '@Churumelass @TukaScaletti Todos preocupados c a Copa e Reeleição Combate ao transmissor da Dengue nada. E olha que são só ações preventivas', 'edits': [{'new': 1, 'date': '2015-05-14T17:59:47.555000', 'curator_id': '555232868624c82a1c6d2ca3', 'field': 'tags'}, {'new': 1, 'old': 1, 'date': '2015-05-16T13:48:47.248000', 'curator_id': '540e300a7673826b17a5604c', 'field': 'tags'}], 'f': 'tw20145130114', 'uid': '57297022', 'cc': 'BR'}\n"
     ]
    }
   ],
   "source": [
    "print(cmntags[174])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '408779568778997760',\n",
      " 'acr': '2011-08-13T00:59:17',\n",
      " 'cc': 'MX',\n",
      " 'cr': '2013-12-05T21:06:58',\n",
      " 'edits': [{'curator_id': '540e300a7673826b17a5604c',\n",
      "            'date': '2015-08-15T21:18:45.875000',\n",
      "            'field': 'tags',\n",
      "            'new': 1}],\n",
      " 'f': 'tw201312520232',\n",
      " 'flng': 213,\n",
      " 'flrs': 399,\n",
      " 'lang': 'es',\n",
      " 'loc': 'Monterrey Es De Tigres ',\n",
      " 'p': 'c5c72776b9f0cc0b',\n",
      " 'pln': -100.273,\n",
      " 'plt': 25.737,\n",
      " 't': '@Doravelizg  es el Dengueeeee',\n",
      " 'tags': {'540e300a7673826b17a5604c': 1},\n",
      " 'tln': -100.277,\n",
      " 'tlt': 25.746,\n",
      " 'uid': '354120599',\n",
      " 'v': False}\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tweet = cmntags[174]\n",
    "print(len(tweet['edits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def getSVMFeatureVectorAndLabels(tweets, featureList):\n",
    "    sortedFeatures = sorted(featureList)\n",
    "    map = {}\n",
    "    feature_vector = []\n",
    "    labels = []\n",
    "    for t in tweets:\n",
    "        label = 0\n",
    "        map = {}\n",
    "        #Initialize empty map\n",
    "        for w in sortedFeatures:\n",
    "            map[w] = 0\n",
    "\n",
    "        tweet_words = t[0]\n",
    "        tweet_tag = t[1]\n",
    "        #Fill the map\n",
    "        for word in tweet_words:\n",
    "            #process the word (remove repetitions and punctuations)\n",
    "            word = replaceTwoOrMore(word)\n",
    "            word = word.strip('\\'\"?,.')\n",
    "            #set map[word] to 1 if word exists\n",
    "            if word in map:\n",
    "                map[word] = 1\n",
    "        #end for loop\n",
    "        values = list(map.values())\n",
    "        feature_vector.append(values)\n",
    "        if(tweet_tag == 1):\n",
    "            label = 1\n",
    "        elif(tweet_tag == 3):\n",
    "            label = 3\n",
    "        labels.append(label)\n",
    "    #return the list of feature_vector and labels\n",
    "    return {'feature_vector' : feature_vector, 'labels': labels}\n",
    "#end\n",
    "\n",
    "def getSVMFeatureVector(tweet_words, featureList):\n",
    "    sortedFeatures = sorted(featureList)\n",
    "    map = {}\n",
    "    feature_vector = []\n",
    "    labels = []\n",
    "    label = 0\n",
    "    map = {}\n",
    "    #Initialize empty map\n",
    "    for w in sortedFeatures:\n",
    "        map[w] = 0\n",
    "\n",
    "    #Fill the map\n",
    "    for word in tweet_words:\n",
    "        #process the word (remove repetitions and punctuations)\n",
    "        word = replaceTwoOrMore(word)\n",
    "        word = word.strip('\\'\"?,.')\n",
    "        #set map[word] to 1 if word exists\n",
    "        if word in map:\n",
    "            map[word] = 1\n",
    "    #end for loop\n",
    "    values = list(map.values())\n",
    "    feature_vector.append(values)\n",
    "    #return the list of feature_vector\n",
    "    return feature_vector\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37486\n",
      "5998\n",
      "Got tweetset\n",
      "Got FVs\n",
      "Trained\n",
      "1259\n",
      "0.8117343649258543\n",
      "39395\n",
      "6413\n",
      "Got tweetset\n",
      "Got FVs\n",
      "Trained\n",
      "1325.811734364926\n",
      "0.8542601381217306\n",
      "39441\n",
      "6348\n",
      "Got tweetset\n",
      "Got FVs\n",
      "Trained\n",
      "1295.8542601381218\n",
      "0.8354959768782217\n",
      "37901\n",
      "6020\n",
      "Got tweetset\n",
      "Got FVs\n",
      "Trained\n",
      "1277.8354959768783\n",
      "0.8233476133871639\n",
      "39925\n",
      "6524\n",
      "Got tweetset\n",
      "Got FVs\n",
      "Trained\n",
      "1355.823347613387\n",
      "0.8735975177921309\n",
      "0.8396871222210203\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "n = 5\n",
    "dsize = len(cmntags)\n",
    "finacc = 0\n",
    "acc = 0\n",
    "\n",
    "for i in range(0,n):\n",
    "    testset = []\n",
    "    trainset = []\n",
    "    ind1 = int(i*dsize/n)\n",
    "    ind2 = int((i+1)*dsize/n)\n",
    "    testset = cmntags[ind1:ind2]\n",
    "    trainset = cmntags[:ind1]\n",
    "    trainset.extend(cmntags[ind2:])\n",
    "\n",
    "    featureList = []\n",
    "    tweetset = []\n",
    "\n",
    "    #Exp\n",
    "    tagList = []\n",
    "\n",
    "    #    trainset = cmntags[0:5999]\n",
    "    for tweet in trainset:\n",
    "        tweetFV = []\n",
    "        text = tweet['t']\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        tag = tweet['tags'][cid]\n",
    "\n",
    "        # Experiment\n",
    "        tagList.append(tag)\n",
    "\n",
    "        processedtext = processTweet(text)\n",
    "        tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "        featureList.extend(tweetFV)\n",
    "        tweetset.append((tweetFV,tag))\n",
    "\n",
    "    print(len(featureList))\n",
    "    featureListSet = set(featureList)\n",
    "    print(len(featureListSet))\n",
    "    featureList = list(featureListSet)\n",
    "\n",
    "    print('Got tweetset')\n",
    "    result = getSVMFeatureVectorAndLabels(tweetset, featureList)\n",
    "    print('Got FVs')\n",
    "    clf = svm.SVC(kernel = 'linear')\n",
    "    feature_vector = result['feature_vector']\n",
    "    labels = result['labels']\n",
    "    clf.fit(feature_vector, labels)\n",
    "    print('Trained')\n",
    "    \n",
    "\n",
    "# acc = 0\n",
    "# finacc = 0\n",
    "\n",
    "    for tweet in testset:\n",
    "        fv = []\n",
    "        # Test the classifier\n",
    "        testTweet = tweet['t']\n",
    "        processedTestTweet = processTweet(testTweet)\n",
    "        fv = getFeatureVector(processedTestTweet,fv)\n",
    "        res = getSVMFeatureVector(fv, featureList)\n",
    "        if (res == []):\n",
    "            continue\n",
    "        x = clf.predict(res)\n",
    "\n",
    "    #         fp.write(testTweet)\n",
    "    #         fp.write('\\t')\n",
    "    #         fp.write(str(x))\n",
    "    #         fp.write('\\t')\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "    #         fp.write(str(tweet['tags'][cid]))\n",
    "    #         fp.write('\\n')\n",
    "        if (x[0] == tweet['tags'][cid]):\n",
    "            acc = acc + 1\n",
    "\n",
    "    # fp.close()\n",
    "    print(acc)\n",
    "    acc = acc/len(testset)\n",
    "    print(acc)\n",
    "    finacc = finacc + acc\n",
    "\n",
    "finacc = finacc/n\n",
    "print(finacc)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "tm = {}\n",
    "tm['t1'] = 0\n",
    "tm['t2'] = 1\n",
    "tm['t3'] = 4\n",
    "val = list(tm.values())\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-8983abe2845b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m't'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# text = \"46 people alive \\m/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mptt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessTweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testset' is not defined"
     ]
    }
   ],
   "source": [
    "tweet = testset[2]\n",
    "text = tweet['t']\n",
    "# text = \"46 people alive \\m/\"\n",
    "print(text)\n",
    "ptt = processTweet(text)\n",
    "print(ptt)\n",
    "words = ptt.split()\n",
    "print(words)\n",
    "for w in words:\n",
    "    u = w.encode('unicode-escape')\n",
    "    print(u)\n",
    "        #replace two or more with two occurrences\n",
    "    w = replaceTwoOrMore(w)\n",
    "    print(w)\n",
    "        #strip punctuation\n",
    "    w = w.strip('\\'\"?,*.')\n",
    "    print(w)    \n",
    "#       check if the word starts with an alphabet\n",
    "    val = re.search(r\"^[a-zA-Z]\", w)\n",
    "    print(val)\n",
    "    val = re.search(b'\\\\U', u)\n",
    "    print(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37486\n",
      "5998\n",
      "[1, 3]\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.320\n",
      "             2          -0.57166        0.680\n",
      "             3          -0.57141        0.680\n",
      "             4          -0.57119        0.680\n",
      "         Final          -0.57098        0.680\n",
      "Training done\n",
      "1043\n",
      "0.6724693745970342\n",
      "0.13449387491940684\n"
     ]
    }
   ],
   "source": [
    "# Training & n-fold Cross Validation of Max Entropy classifier\n",
    "\n",
    "n = 5\n",
    "dsize = len(cmntags)\n",
    "finacc = 0\n",
    "fp = open('featureList.txt','w')\n",
    "\n",
    "for i in range(0,1):\n",
    "    testset = []\n",
    "    trainset = cmntags\n",
    "#     ind1 = int(i*dsize/n)\n",
    "#     ind2 = int((i+1)*dsize/n)\n",
    "#     testset = cmntags[ind1:ind2]\n",
    "#     trainset = cmntags[:ind1]\n",
    "#     trainset.extend(cmntags[ind2:])\n",
    "    \n",
    "    featureList = []\n",
    "    tweetset = []\n",
    "    \n",
    "    #Exp\n",
    "    tagList = []\n",
    "    \n",
    "#    trainset = cmntags[0:5999]\n",
    "    for tweet in trainset:\n",
    "        tweetFV = []\n",
    "        text = tweet['t']\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        tag = tweet['tags'][cid]\n",
    "        \n",
    "        # Experiment\n",
    "        tagList.append(tag)\n",
    "        \n",
    "        processedtext = processTweet(text)\n",
    "        tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "        fp.write(' '.join(tweetFV))\n",
    "        featureList.extend(tweetFV)\n",
    "        tweetset.append((tweetFV,tag))\n",
    "\n",
    "    print(len(featureList))\n",
    "    featureListSet = set(featureList)\n",
    "    print(len(featureListSet))\n",
    "    featureList = list(featureListSet)\n",
    "    \n",
    "    tagListSet = set(tagList)\n",
    "    tagList = list(tagListSet)\n",
    "    print(tagList)\n",
    "\n",
    "    # Extract feature vector for all tweets in one shote\n",
    "    training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "    # Train the classifier\n",
    "    MaxEntClassifier = nltk.classify.maxent.MaxentClassifier.train(training_set, 'GIS', trace=3, \\\n",
    "    encoding=None, labels=None, gaussian_prior_sigma=0, max_iter = 5)\n",
    "    print('Training done')\n",
    "    \n",
    "    acc = 0\n",
    "    for tweet in testset:\n",
    "        fv = []\n",
    "        # Test the classifier\n",
    "        testTweet = tweet['t']\n",
    "        processedTestTweet = processTweet(testTweet)\n",
    "        x = MaxEntClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "#         fp.write(testTweet)\n",
    "#         fp.write('\\t')\n",
    "#         fp.write(str(x))\n",
    "#         fp.write('\\t')\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "#         fp.write(str(tweet['tags'][cid]))\n",
    "#         fp.write('\\n')\n",
    "        if (x == tweet['tags'][cid]):\n",
    "            acc = acc + 1\n",
    "\n",
    "# fp.close()\n",
    "    print(acc)\n",
    "    acc = acc/len(testset)\n",
    "    print(acc)\n",
    "    finacc = finacc + acc\n",
    "\n",
    "finacc = finacc/n\n",
    "print(finacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in data:\n",
    "        tweet = cmntags[i]\n",
    "        cid1 = tweet['edits'][0]['curator_id']\n",
    "        cid2 = tweet['edits'][0]['curator_id']\n",
    "        tag1 = tweet['tags'][cid1]\n",
    "        tag2 = tweet['tags'][cid1]\n",
    "        if (tag1 == 0)|(tag2 == 0):\n",
    "            pprint(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ToKi9llAMockingbird', 'onemore']\n",
      "7\n",
      "Ki9ll\n",
      "8\n",
      "A\n",
      "['To', 'Ki9ll', 'A', 'Mockingbird', 'onemore']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "t = \"blah #ToKi9llAMockingbird #onemore blah\"\n",
    "# t1 = re.match(r'#([^\\s])', t)\n",
    "# t = \"#A\"\n",
    "regex = re.compile(r'#([^\\s]+)')\n",
    "matchObj = regex.findall(t)\n",
    "print(matchObj)\n",
    "s = len(matchObj)\n",
    "fv = []\n",
    "for i in range(0,s):\n",
    "    word = matchObj[i]\n",
    "    startInd = len(word)\n",
    "    stopInd = 0\n",
    "    for i in range(0,len(word)):\n",
    "        if (i==(len(word)-1)):\n",
    "                    stopInd = i + 1\n",
    "                    if (startInd == len(word)):\n",
    "                        startInd = 0\n",
    "                    fv.append(word[startInd:stopInd])\n",
    "                    continue\n",
    "        if (word[i].isupper()):\n",
    "            if (startInd != len(word)):\n",
    "                stopInd = i\n",
    "                print(i)\n",
    "                print(word[startInd:stopInd])\n",
    "                fv.append(word[startInd:stopInd])\n",
    "                startInd = i\n",
    "            else:\n",
    "                if (i != 0):\n",
    "                    startInd = 0\n",
    "                    stopInd = i\n",
    "                    fv.append(word[startInd:stopInd])\n",
    "                    startInd = i\n",
    "                \n",
    "print(fv)\n",
    "        \n",
    "        \n",
    "# matchObj = re.sub(r'#([^\\s]+)',r'\\1',matchObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhaahahhahahaa\n"
     ]
    }
   ],
   "source": [
    "s = \"hhaahahhahahaa\"\n",
    "pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "s = pattern.sub(r\"\\1\\1\", s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61316\n",
      "13924\n",
      "[1, 3]\n",
      "FVs done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testset = difftags\n",
    "# trainset = []\n",
    "# ind1 = int(i*dsize/n)\n",
    "# ind2 = int((i+1)*dsize/n)\n",
    "# testset = cmntags[ind1:ind2]\n",
    "trainset = cmntags\n",
    "# trainset.extend(cmntags[ind2:])\n",
    "\n",
    "featureList = []\n",
    "tweetset = []\n",
    "\n",
    "#Exp\n",
    "tagList = []\n",
    "\n",
    "#    trainset = cmntags[0:5999]\n",
    "for tweet in trainset:\n",
    "    tweetFV = []\n",
    "    text = tweet['t']\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    tag = tweet['tags'][cid]\n",
    "\n",
    "    # Experiment\n",
    "    tagList.append(tag)\n",
    "\n",
    "    processedtext = processTweet(text)\n",
    "    tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "    featureList.extend(tweetFV)\n",
    "    for i in range(0,len(tweetID)):\n",
    "        if (tweetID[i] == tweet['_id']):\n",
    "            tweetFV.extend(region[i])\n",
    "            tweetFV.extend(state[i])\n",
    "            tweetFV.extend(county[i])\n",
    "            tweetFV.extend(mregion[i])\n",
    "            break\n",
    "\n",
    "    tweetset.append((tweetFV,tag))\n",
    "\n",
    "featureList.extend(list(regionSet))\n",
    "featureList.extend(list(stateSet))\n",
    "featureList.extend(list(countySet))\n",
    "featureList.extend(list(mregionSet))\n",
    "featureList.extend(list(stateSet))\n",
    "print(len(featureList))\n",
    "featureListSet = set(featureList)\n",
    "print(len(featureListSet))\n",
    "featureList = list(featureListSet)\n",
    "#     flist.write(\"\\n\".join(featureList))\n",
    "\n",
    "tagListSet = set(tagList)\n",
    "tagList = list(tagListSet)\n",
    "print(tagList)\n",
    "print('FVs done')\n",
    "\n",
    "# Extract feature vector for all tweets in one shote\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "# Train the classifier\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "fn = 0\n",
    "fp = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "for tweet in testset:\n",
    "    fv = []\n",
    "    # Test the classifier\n",
    "    testTweet = tweet['t']\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    tweet['ctags'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = difftags\n",
    "for tweet in testset:\n",
    "    fv = []\n",
    "    # Test the classifier\n",
    "    testTweet = tweet['t']\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    tweet['ctags'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101\n",
      "1106\n",
      "2207\n"
     ]
    }
   ],
   "source": [
    "count1 = 0\n",
    "count3 = 0\n",
    "for tweet in testset:\n",
    "        if (tweet['ctags'] == 1):\n",
    "            count1 = count1 + 1\n",
    "        if (tweet['ctags'] == 3):\n",
    "            count3 = count3 + 1\n",
    "print(count1)\n",
    "print(count3)\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6548\n",
      "4074\n",
      "10622\n",
      "10622\n"
     ]
    }
   ],
   "source": [
    "print(count1)\n",
    "print(count3)\n",
    "print(len(testset))\n",
    "cache_testdata = testset\n",
    "print(len(cache_testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('tweet_master_data.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# hello = [['Me','You'],['293', '219'],['13','15']]\n",
    "hello = testset[0:3]\n",
    "length = len(hello[0])\n",
    "\n",
    "with open('test1.csv', 'w') as testfile:\n",
    "    csv_writer = csv.writer(testfile)\n",
    "#     for y in range(length):\n",
    "    csv_writer.writerow([x['_id'] for x in hello])\n",
    "    csv_writer.writerow([x['cc'] for x in hello])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "fp = open('test.csv', 'w', newline='')\n",
    "a = csv.writer(fp, delimiter=',')\n",
    "a.writerow(('Time Stamp', 'Tweet Longitude', 'Tweet Latitude', 'Country', 'Region', 'State', 'County', 'Microregion'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = []\n",
    "with open('tweet_master_data.json') as f:\n",
    "    for line in f:\n",
    "        data2.extend(json.loads(line))\n",
    "        \n",
    "dsize = len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2955\n",
      "9316\n",
      "23\n",
      "13512\n"
     ]
    }
   ],
   "source": [
    "count3cmn = 0\n",
    "count3diff = 0\n",
    "count2cmn = 0\n",
    "countcmn = 0\n",
    "for i in range(0,len(data2)):\n",
    "    tweet = data2[i]\n",
    "    if (tweet['lang'] == 'es')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for j in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][j]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        countcmn = countcmn + 1\n",
    "        if (tweet['tags'][cid1] == 3):\n",
    "            count3cmn = count3cmn + 1\n",
    "        if (tweet['tags'][cid1] == 2):\n",
    "            count2cmn = count2cmn + 1\n",
    "#     else:\n",
    "#         if(tweet['ctags'] == 3):\n",
    "#             count3diff = count3diff + 1\n",
    "print(count3cmn)\n",
    "print(countcmn)\n",
    "print(count2cmn)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        \n",
    "for i in range(0,dsize):\n",
    "    tweet = data2[i]\n",
    "    if (tweet['lang'] == 'es')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for j in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][j]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        continue\n",
    "#         print('Common Tags')\n",
    "#         if (tweet['tags'][cid1] == 3):\n",
    "#             print('Sickness Tweet')\n",
    "#             a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))\n",
    "    else:\n",
    "#         print('Different Tags')\n",
    "        testTweet = tweet['t']\n",
    "        processedTestTweet = processTweet(testTweet)\n",
    "        x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "        data2[i].update({'ctags':x})\n",
    "#         print('Classified')\n",
    "#         if (x == 3):\n",
    "#             print('Sickness Tweet')\n",
    "#             a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))       \n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '309822840889303040',\n",
      " 'cc': 'BR',\n",
      " 'country': 'Brasil',\n",
      " 'county': 'Campo Mourão',\n",
      " 'cr': '2013-03-07T19:28:16',\n",
      " 'ctags': 1,\n",
      " 'edits': [{'curator_id': '540e300a7673826b17a5604c',\n",
      "            'date': '2015-05-25T21:57:43.353000',\n",
      "            'field': 'tags',\n",
      "            'new': 3},\n",
      "           {'curator_id': '555232868624c82a1c6d2ca3',\n",
      "            'date': '2015-07-06T17:30:41.279000',\n",
      "            'field': 'tags',\n",
      "            'new': 1,\n",
      "            'old': 1}],\n",
      " 'f': 'tw201337184414',\n",
      " 'lang': 'pt',\n",
      " 'microregion': 'Microrregião Campo Mourão',\n",
      " 'p': '0ababd6f282787a7',\n",
      " 'pln': -52.366,\n",
      " 'plt': -24.126,\n",
      " 'region': 'Região Sul',\n",
      " 'state': 'PR',\n",
      " 't': 'tomara q eu esteja c dengue',\n",
      " 'tags': {'540e300a7673826b17a5604c': 3, '555232868624c82a1c6d2ca3': 1},\n",
      " 'tln': -52.379,\n",
      " 'tlt': -24.05,\n",
      " 'uid': '109335884',\n",
      " 'v': True}\n"
     ]
    }
   ],
   "source": [
    "pprint(data2[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2955\n",
      "0\n",
      "13512\n"
     ]
    }
   ],
   "source": [
    "count3cmn = 0\n",
    "count3diff = 0\n",
    "for i in range(0,len(data2)):\n",
    "    tweet = data2[i]\n",
    "    if (tweet['lang'] == 'es')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for j in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][j]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        if (tweet['tags'][cid1] == 3):\n",
    "            count3cmn = count3cmn + 1\n",
    "    else:\n",
    "        if(tweet['ctags'] == 3):\n",
    "            count3diff = count3diff + 1\n",
    "print(count3cmn)\n",
    "print(count3diff)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '309822840889303040',\n",
      " 'cc': 'BR',\n",
      " 'country': 'Brasil',\n",
      " 'county': 'Campo Mourão',\n",
      " 'cr': '2013-03-07T19:28:16',\n",
      " 'edits': [{'curator_id': '540e300a7673826b17a5604c',\n",
      "            'date': '2015-05-25T21:57:43.353000',\n",
      "            'field': 'tags',\n",
      "            'new': 3},\n",
      "           {'curator_id': '555232868624c82a1c6d2ca3',\n",
      "            'date': '2015-07-06T17:30:41.279000',\n",
      "            'field': 'tags',\n",
      "            'new': 1,\n",
      "            'old': 1}],\n",
      " 'f': 'tw201337184414',\n",
      " 'lang': 'pt',\n",
      " 'microregion': 'Microrregião Campo Mourão',\n",
      " 'p': '0ababd6f282787a7',\n",
      " 'pln': -52.366,\n",
      " 'plt': -24.126,\n",
      " 'region': 'Região Sul',\n",
      " 'state': 'PR',\n",
      " 't': 'tomara q eu esteja c dengue',\n",
      " 'tags': {'540e300a7673826b17a5604c': 3, '555232868624c82a1c6d2ca3': 1},\n",
      " 'tln': -52.379,\n",
      " 'tlt': -24.05,\n",
      " 'uid': '109335884',\n",
      " 'v': True}\n"
     ]
    }
   ],
   "source": [
    "pprint(data2[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
