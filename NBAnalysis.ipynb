{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Python's json Encoder and Decoder\n",
    "import json\n",
    "from pprint import pprint # Pretty Print\n",
    "\n",
    "# Parsing of json file line-by-line\n",
    "# Name of file containing validated dengue tweets: 'dengueDump_3.23.16_validated.json'\n",
    "# Name of file containing validated dengue tweets: 'dengueDump_3.23.16_nonvalidated.json'\n",
    "# data = []\n",
    "# with open('tweet_master_data.json') as f:\n",
    "#     for line in f:\n",
    "#         data.append(json.loads(line))\n",
    "        \n",
    "# Parsing of json file as one list\n",
    "# Name of file which combines location info with validated dengue tweets: 'tweet_master_data.json'\n",
    "data = []\n",
    "with open('tweet_master_data.json') as jdata:\n",
    "    data = json.load(jdata)\n",
    "    jdata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13513\n"
     ]
    }
   ],
   "source": [
    "# Check the size of dataset\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a tweet data entity:\n",
    "\n",
    "{'_id': '299714679532556288',\n",
    " 'cc': 'BR',\n",
    " 'country': 'Brasil',\n",
    " 'county': 'RondonÃ³polis',\n",
    " 'cr': '2013-02-07T22:02:02',\n",
    " 'edits': [{'curator_id': '540e300a7673826b17a5604c',\n",
    "            'date': '2015-07-06T01:18:27.900000',\n",
    "            'field': 'tags',\n",
    "            'new': 1},\n",
    "           {'curator_id': '555232868624c82a1c6d2ca3',\n",
    "            'date': '2015-07-06T17:40:18.407000',\n",
    "            'field': 'tags',\n",
    "            'new': 1,\n",
    "            'old': 1}],\n",
    " 'f': 'tw2013272123',\n",
    " 'lang': 'pt',\n",
    " 'loc': ' MT / PR',\n",
    " 'microregion': 'MicrorregiÃ£o de RondonÃ³polis',\n",
    " 'p': '48401b8f7232dfb8',\n",
    " 'pln': -54.607,\n",
    " 'plt': -16.572,\n",
    " 'region': 'RegiÃ£o Centro-Oeste',\n",
    " 'state': 'MT',\n",
    " 't': 'Dengue ðŸ˜«',\n",
    " 'tags': {'540e300a7673826b17a5604c': 1, '555232868624c82a1c6d2ca3': 1},\n",
    " 'tln': -54.649,\n",
    " 'tlt': -16.463,\n",
    " 'uid': '419780633',\n",
    " 'v': True}\n",
    "\n",
    "Keys in json file:\n",
    "1. v - validated (true/false)\n",
    "2. tags - dictionary with key (curator ID) and value (label).  The labels are as follows: \n",
    "1=junk, 2=report, 3=sickness\n",
    "3. edits - a dictionary that keeps track of all tags applied by curators.  Not really useful unless you want to see if somebody is re-rating tweets or if they erase tags by accident.  \n",
    "\n",
    "More keys in json file:\n",
    "    \"_id\" : tweet ID (also the object ID for the mongo db)\n",
    "    \"lang\" : language of tweet\n",
    "    \"loc\" : user-entered location name\n",
    "    \"plt\" : profile latitude coordinates\n",
    "    \"pln\" : profile longitude\n",
    "    \"uid\" : twitter user id\n",
    "    \"tlt\" : tweet latitude\n",
    "    \"tln\" : tweet longitude\n",
    "    \"cc\" : country code\n",
    "    \"f\" : our own backup coding\n",
    "    \"p\" : twitter place ID (not sure if these can be looked up somehow via twitter) \n",
    "    \"t\" : tweet text\n",
    "    \"acr\": time of the userâ€™s account creation in UTC\n",
    "    \"cr\" : time of the tweet in UTC \n",
    "    \"flrs\": number of followers\n",
    "    \"flng\" : number of accounts following (friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10116\n"
     ]
    }
   ],
   "source": [
    "# Extraction of portuguese tweets\n",
    "pt_tweets = []\n",
    "for tweet in data:\n",
    "    # Portuguese Tweets are encoded as 'pt' and spanish tweets are encoded as 'es' under the key 'lang'\n",
    "    if tweet['lang'] == 'pt':\n",
    "        pt_tweets.append(tweet)\n",
    "\n",
    "# Check the size of dataset containing only portuguese tweets\n",
    "print(len(pt_tweets))\n",
    "# Release memory of redundant variables\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9965\n"
     ]
    }
   ],
   "source": [
    "# Extraction of tweets tagged as junk(1) or sickness(3) only\n",
    "pt13_tweets = []\n",
    "for tweet in pt_tweets:\n",
    "    # Identify curator ids of each tweet to refer their tags\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for i in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][i]['curator_id'])\n",
    "    cidListSet = set(cidList) # Eliminates redundancy in set elements\n",
    "    cidList = list(cidListSet) \n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    if (tweet['tags'][cid1] != 2 and tweet['tags'][cid2] != 2):\n",
    "        pt13_tweets.append(tweet)\n",
    "\n",
    "# Check the size of dataset containing portuguese tweets tagged as 'junk' or 'sickness' only\n",
    "print(len(pt13_tweets))\n",
    "# Release memory of redundant variables\n",
    "del pt_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7758\n",
      "2207\n"
     ]
    }
   ],
   "source": [
    "# Differentiating tweets into those with common tags and those with different tags - easier for manipulation\n",
    "\n",
    "cmntags = [] # For tweets with annotators' agreement\n",
    "difftags = [] # For tweets with annotators' diasgreement\n",
    "zerotag = []\n",
    "for tweet in pt13_tweets:\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for i in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][i]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        cmntags.append(tweet)\n",
    "    else:\n",
    "        difftags.append(tweet)\n",
    "    \n",
    "#     # Tweets tagged as (0) - System Error\n",
    "#     if ((tweet['tags'][cid1] == 0)|(tweet['tags'][cid2] == 0)):\n",
    "#         zerotag.append(tweet)\n",
    "\n",
    "print(len(cmntags)) # Check size of \n",
    "print(len(difftags))\n",
    "del pt13_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only those tweets with annotators' agreement are considered for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'junk' tweets in training set: 5261\n",
      "Number of 'sickness' tweets in training set: 2497\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of tweets tagged as junk and sickness \n",
    "count3 = 0\n",
    "for tweet in cmntags:\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    if(tweet['tags'][cid] == 3):\n",
    "        count3 = count3 + 1\n",
    "# end\n",
    "print(\"Number of 'junk' tweets in training set:\", end = \" \")\n",
    "print(len(cmntags)-count3)\n",
    "print(\"Number of 'sickness' tweets in training set:\", end = \" \")\n",
    "print(count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Define a function for pre-processing of tweets\n",
    "def processTweet(tweet):\n",
    "\n",
    "    # Convert to lower case\n",
    "#     tweet = tweet.lower()\n",
    "    # Convert hyperlinks to a generic term 'URL' or an empty space\n",
    "#     tweet = re.sub('((www\\.[^\\s]+)|(https?:\\/\\/[^\\s]+))','URL',tweet)\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?:\\/\\/[^\\s]+))','',tweet)\n",
    "    # Convert @username to USER or an empty space\n",
    "#     tweet = re.sub('(@[^\\s]+)|(@[\\s][^\\s]+)','USER',tweet)\n",
    "    tweet = re.sub('(@[^\\s]+)|(@[\\s][^\\s]+)','',tweet)\n",
    "    # Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    # Replace #word with word\n",
    "#     tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    # Trim special charaters\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import NLTK library and portuguese components for manipulation of tweets\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "# print(stopwords[0:10])\n",
    "stopwords.append('USER')\n",
    "stopwords.append('URL')\n",
    "\n",
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.*)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "#start getfeatureVector\n",
    "def getFeatureVector(tweet, featureVector):\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = re.sub(r'#([^\\s]+)', r'\\1', w)\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,*.(_!)/')\n",
    "        w = w.replace('\\\\','')\n",
    "        w = w.replace('/','')\n",
    "        \n",
    "        # check if the word starts with an alphabet\n",
    "        # val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        val = re.search(r\"^[a-zA-Z]\", w)\n",
    "\n",
    "        # check if the word contains only numbers\n",
    "        valnum = re.search(r\"^[0-9][0-9]*$\", w)\n",
    "    \n",
    "        # Addition of emoticons to feature list - Commment the next paragraph to omit emojis\n",
    "        u = w.encode('unicode-escape')\n",
    "        bval = re.search(b'\\\\U', u)\n",
    "        if (bval):\n",
    "            s = u.split(b'\\\\U')\n",
    "            for l in range(1,len(s)):\n",
    "                a = (b'\\\\U' + s[l])\n",
    "                astr = a.decode('unicode-escape')\n",
    "                featureVector.append(astr)\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        # ignore if it is a stop word or the word contains only numbers or the word does not start with an alphabet\n",
    "        if (w in stopwords or (val is None) or valnum)\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "            \n",
    "    featureVectorSet = set(featureVector) # Eliminate redundant features\n",
    "    featureVector = list(featureVectorSet) \n",
    "    del featureVectorSet\n",
    "    \n",
    "    # Addition of Bigrams as features        \n",
    "#     for j in range(0,len(featureVector)-1):\n",
    "#         featureVector.append((featureVector[j],featureVector[j+1]))\n",
    "        \n",
    "    # Addition of Trigrams as features        \n",
    "#     for j in range(0,len(featureVector)-2):\n",
    "#         featureVector.append((featureVector[j],featureVector[j+1],featureVector[j+2]))\n",
    "    \n",
    "    # Break down a hashtag into individual words whenever possible to get additional cues about topic of the tweet\n",
    "    # Example: Hashtag1 - #ifyouknowwhatimean, Hashtag2 - #IfYouKnowWhatIMean\n",
    "    # Hashtag1 can not be analysed further without a dictionary, but Hashtag2 can be fragmented at the capital letters            \n",
    "    regex = re.compile(r'#([^\\s]+)')\n",
    "    matchObj = regex.findall(tweet)\n",
    "#     print(matchObj)\n",
    "    s = len(matchObj) # Multiple hashtags\n",
    "    fv = []\n",
    "    for i in range(0,s):\n",
    "        word = matchObj[i]\n",
    "        # Initialization\n",
    "        startInd = len(word)\n",
    "        stopInd = 0\n",
    "        for i in range(0,len(word)):\n",
    "            if (i==(len(word)-1)):\n",
    "                        stopInd = i + 1\n",
    "                        if (startInd == len(word)):\n",
    "                            startInd = 0\n",
    "                        # Single capital letter identified (Example: 'I')\n",
    "                        fv.append(word[startInd:stopInd].lower()) # Single Capital Letter identified at the end of tag\n",
    "                        continue\n",
    "            if (word[i].isupper()):\n",
    "                if (startInd != len(word)):\n",
    "                    stopInd = i\n",
    "#                     print(i)\n",
    "#                     print(word[startInd:stopInd])\n",
    "                    # Word identified within the phrase\n",
    "                    fv.append(word[startInd:stopInd].lower()) # \n",
    "                    startInd = i\n",
    "                else:\n",
    "                    if (i != 0):\n",
    "                        startInd = 0\n",
    "                        stopInd = i\n",
    "                        # Word identified at the starting of the phrase\n",
    "                        fv.append(word[startInd:stopInd].lower())\n",
    "                        startInd = i\n",
    "    \n",
    "    featureVector.extend(fv)\n",
    "    return featureVector\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14611"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparation of test data\n",
    "data = []\n",
    "with open('dengueDump_3.23.16_nonvalidated.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pt_tweets = []\n",
    "for tweet in data:\n",
    "    # Portuguese Tweets are encoded as 'pt' and spanish tweets are encoded as 'es'\n",
    "    if tweet['lang'] == 'pt':\n",
    "        pt_tweets.append(tweet)\n",
    "\n",
    "len(pt_tweets)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start extract_features\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        if (type(word) is tuple):\n",
    "            temp = 'contains' + str(word)\n",
    "            features[temp] = (word in tweet_words)\n",
    "            del temp\n",
    "        else:\n",
    "            features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43165\n",
      "7900\n",
      "FVs done\n",
      "Round 0 in cross validation\n",
      "Accuracy: 0.8201160541586073\n",
      "Precision: 0.7026548672566372\n",
      "Recall: 0.781496062992126\n",
      "TNR: 0.8389261744966443\n",
      "45417\n",
      "8512\n",
      "FVs done\n",
      "Round 1 in cross validation\n",
      "Accuracy: 0.8685567010309279\n",
      "Precision: 0.7148760330578512\n",
      "Recall: 0.8398058252427184\n",
      "TNR: 0.8789473684210526\n",
      "45594\n",
      "8438\n",
      "FVs done\n",
      "Round 2 in cross validation\n",
      "Accuracy: 0.8413926499032882\n",
      "Precision: 0.747557003257329\n",
      "Recall: 0.8345454545454546\n",
      "TNR: 0.8451548451548452\n",
      "43414\n",
      "7895\n",
      "FVs done\n",
      "Round 3 in cross validation\n",
      "Accuracy: 0.8344072164948454\n",
      "Precision: 0.8003913894324853\n",
      "Recall: 0.725177304964539\n",
      "TNR: 0.8967611336032388\n",
      "45982\n",
      "8667\n",
      "FVs done\n",
      "Round 4 in cross validation\n",
      "Accuracy: 0.8885309278350515\n",
      "Precision: 0.7947154471544715\n",
      "Recall: 0.8444924406047516\n",
      "TNR: 0.9072543617998163\n",
      "Final Accuracy: 0.8506007098845441\n",
      "Final Precision: 0.7520389480317549\n",
      "Final Recall: 0.8051034176699179\n",
      "Final True Negative Rate: 0.8734087766951195\n"
     ]
    }
   ],
   "source": [
    "# Training & n-fold Cross Validation of Naive Bayes classifier\n",
    "\n",
    "count = 0\n",
    "n = 5\n",
    "dsize = len(cmntags)\n",
    "finacc = 0\n",
    "finprec = 0\n",
    "fintnr = 0\n",
    "finrecall = 0\n",
    "# flist = open('featureList.txt','w')\n",
    "\n",
    "for i in range(0,n):\n",
    "    testset = []\n",
    "    trainset = []\n",
    "    ind1 = int(i*dsize/n)\n",
    "    ind2 = int((i+1)*dsize/n)\n",
    "    testset = cmntags[ind1:ind2]\n",
    "    trainset = cmntags[:ind1]\n",
    "    trainset.extend(cmntags[ind2:])\n",
    "    \n",
    "    featureList = []\n",
    "    tweetset = []\n",
    "    \n",
    "    #Exp\n",
    "    tagList = []\n",
    "    \n",
    "#    trainset = cmntags[0:5999]\n",
    "    for tweet in trainset:\n",
    "        tweetFV = []\n",
    "        text = tweet['t']\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        tag = tweet['tags'][cid]\n",
    "        \n",
    "        processedtext = processTweet(text)\n",
    "        tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "        \n",
    "        # Addition of location data to feature vector - Comment the next 4 lines to omit location data form features\n",
    "#         tweetFV.append(tweet['region'])\n",
    "#         tweetFV.append(tweet['state'])\n",
    "#         tweetFV.append(tweet['county'])\n",
    "#         tweetFV.append(tweet['microregion'])\n",
    "        \n",
    "        tweetFVSet = set(tweetFV)\n",
    "        tweetFV = list(tweetFVSet)\n",
    "        del tweetFVSet\n",
    "        featureList.extend(tweetFV)\n",
    "        tweetset.append((tweetFV,tag))\n",
    "\n",
    "    print(len(featureList))\n",
    "    featureListSet = set(featureList)\n",
    "    print(len(featureListSet))\n",
    "    featureList = list(featureListSet)\n",
    "    featureListStr = [str(item) for item in featureList]\n",
    "#     flist.write(\"\\t\".join(featureListStr))\n",
    "    \n",
    "    print('FVs done')\n",
    "\n",
    "    # Extract feature vector for all tweets in one shote\n",
    "    training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "    # Train the classifier\n",
    "    NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    \n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "\n",
    "    for j in range(0,len(testset)):\n",
    "        tweet = testset[j]\n",
    "        fv = []\n",
    "        # Test the classifier\n",
    "        testTweet = tweet['t']\n",
    "        processedTestTweet = processTweet(testTweet)\n",
    "        x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "        pt_tweets[j]['ctags'] = x\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        if (x == tweet['tags'][cid]):\n",
    "            if (x == 1):\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                tp = tp + 1\n",
    "        else:\n",
    "            if (x == 1):\n",
    "                fn = fn + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "\n",
    "    acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "    prec = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    tnr = tn/(tn + fp)\n",
    "    print(\"Round\", i, \"in cross validation\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"TNR:\", tnr)\n",
    "    finacc = finacc + acc\n",
    "    finrecall = finrecall + recall\n",
    "    finprec = finprec + prec\n",
    "    fintnr = fintnr + tnr\n",
    "\n",
    "finacc = finacc/n\n",
    "finrecall = finrecall/n\n",
    "finprec = finprec/n\n",
    "fintnr = fintnr/n\n",
    "print(\"Final Accuracy:\", finacc)\n",
    "print(\"Final Precision:\", finprec)\n",
    "print(\"Final Recall:\", finrecall)\n",
    "print(\"Final True Negative Rate:\", fintnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "72\n",
      "391\n",
      "988\n"
     ]
    }
   ],
   "source": [
    "print(fp)\n",
    "print(fn)\n",
    "print(tp)\n",
    "print(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55893\n",
      "9574\n",
      "FVs done\n"
     ]
    }
   ],
   "source": [
    "# To print the most informative features\n",
    "\n",
    "dsize = len(cmntags)\n",
    "flist = open('featureList.txt','w')\n",
    "\n",
    "trainset = cmntags\n",
    "\n",
    "featureList = []\n",
    "tweetset = []\n",
    "\n",
    "\n",
    "for tweet in trainset:\n",
    "    tweetFV = []\n",
    "    text = tweet['t']\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    tag = tweet['tags'][cid]\n",
    "\n",
    "    processedtext = processTweet(text)\n",
    "    tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "    \n",
    "    # Addition of location data to feature vectors - commment the next 4 lines to omit location data from features\n",
    "#     tweetFV.append(tweet['region'])\n",
    "#     tweetFV.append(tweet['state'])\n",
    "#     tweetFV.append(tweet['county'])\n",
    "#     tweetFV.append(tweet['microregion'])\n",
    "    \n",
    "    tweetFVSet = set(tweetFV)\n",
    "    tweetFV = list(tweetFVSet)\n",
    "    del tweetFVSet\n",
    "    featureList.extend(tweetFV)\n",
    "    tweetset.append((tweetFV,tag))\n",
    "\n",
    "print(len(featureList))\n",
    "featureListSet = set(featureList)\n",
    "print(len(featureListSet))\n",
    "featureList = list(featureListSet)\n",
    "featureListStr = [str(item) for item in featureList]\n",
    "flist.write(\"\\t\".join(featureListStr))\n",
    "\n",
    "print('FVs done')\n",
    "\n",
    "# Extract feature vector for all tweets in one shote\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "# Train the classifier\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print informative features about the classifier\n",
    "print(NBClassifier.show_most_informative_features(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write tweets into .csv files for \n",
    "# data2 = []\n",
    "# with open('tweet_master_data.json') as f:\n",
    "#     for line in f:\n",
    "#         data2.extend(json.loads(line))\n",
    "\n",
    "import csv\n",
    "fp = open('brazil_sickness_tweets_val_NB.csv', 'w', newline='')\n",
    "a = csv.writer(fp, delimiter=',')\n",
    "a.writerow(('Time Stamp', 'Tweet Longitude', 'Tweet Latitude', 'Country', 'Region', 'State', 'County', 'Microregion'))\n",
    "\n",
    "# Writing of commonly annotated sickness tweets into the csv file\n",
    "for k in range(0,len(cmntags)):\n",
    "    tweet = cmntags[k]\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    tag = tweet['tags'][cid]\n",
    "    if(tag == 3):\n",
    "        a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classification of tweets annotated differently by different annotators\n",
    "for k in range(0,len(difftags)):\n",
    "    tweet = difftags[k]\n",
    "    if (tweet['lang'] != 'pt')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    testTweet = tweet['t']\n",
    "    fv = []\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    if (x == 3):\n",
    "        a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = []\n",
    "with open('tweet_master_data2.json') as f:\n",
    "    for line in f:\n",
    "        data2.extend(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification of non-validated tweets\n",
    "for k in range(0,len(data2)):\n",
    "    tweet = data2[k]\n",
    "    if (tweet['lang'] != 'pt')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    testTweet = tweet['t']\n",
    "    fv = []\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    if (x == 3):\n",
    "        a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = \"Ss #rksa pod\"\n",
    "tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "print(tweet)\n",
    "\n",
    "f = open(\"Results-1-FV.txt\",\"w\")\n",
    "# print(featureList, file=f)\n",
    "f.writelines([\"%s\\t\" % item  for item in featureList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def getSVMFeatureVectorAndLabels(tweets, featureList):\n",
    "    sortedFeatures = sorted(featureList)\n",
    "    map = {}\n",
    "    feature_vector = []\n",
    "    labels = []\n",
    "    for t in tweets:\n",
    "        label = 0\n",
    "        map = {}\n",
    "        #Initialize empty map\n",
    "        for w in sortedFeatures:\n",
    "            map[w] = 0\n",
    "\n",
    "        tweet_words = t[0]\n",
    "        tweet_tag = t[1]\n",
    "        #Fill the map\n",
    "        for word in tweet_words:\n",
    "            #process the word (remove repetitions and punctuations)\n",
    "            word = replaceTwoOrMore(word)\n",
    "            word = word.strip('\\'\"?,.')\n",
    "            #set map[word] to 1 if word exists\n",
    "            if word in map:\n",
    "                map[word] = 1\n",
    "        #end for loop\n",
    "        values = list(map.values())\n",
    "        feature_vector.append(values)\n",
    "        if(tweet_tag == 1):\n",
    "            label = 1\n",
    "        elif(tweet_tag == 3):\n",
    "            label = 3\n",
    "        labels.append(label)\n",
    "    #return the list of feature_vector and labels\n",
    "    return {'feature_vector' : feature_vector, 'labels': labels}\n",
    "#end\n",
    "\n",
    "def getSVMFeatureVector(tweet_words, featureList):\n",
    "    sortedFeatures = sorted(featureList)\n",
    "    map = {}\n",
    "    feature_vector = []\n",
    "    labels = []\n",
    "    label = 0\n",
    "    map = {}\n",
    "    #Initialize empty map\n",
    "    for w in sortedFeatures:\n",
    "        map[w] = 0\n",
    "\n",
    "    #Fill the map\n",
    "    for word in tweet_words:\n",
    "        #process the word (remove repetitions and punctuations)\n",
    "        word = replaceTwoOrMore(word)\n",
    "        word = word.strip('\\'\"?,.')\n",
    "        #set map[word] to 1 if word exists\n",
    "        if word in map:\n",
    "            map[word] = 1\n",
    "    #end for loop\n",
    "    values = list(map.values())\n",
    "    feature_vector.append(values)\n",
    "    #return the list of feature_vector\n",
    "    return feature_vector\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM\n",
    "n = 5\n",
    "dsize = len(cmntags)\n",
    "finacc = 0\n",
    "acc = 0\n",
    "\n",
    "for i in range(0,n):\n",
    "    testset = []\n",
    "    trainset = []\n",
    "    ind1 = int(i*dsize/n)\n",
    "    ind2 = int((i+1)*dsize/n)\n",
    "    testset = cmntags[ind1:ind2]\n",
    "    trainset = cmntags[:ind1]\n",
    "    trainset.extend(cmntags[ind2:])\n",
    "\n",
    "    featureList = []\n",
    "    tweetset = []\n",
    "\n",
    "    #Exp\n",
    "    tagList = []\n",
    "\n",
    "    #    trainset = cmntags[0:5999]\n",
    "    for tweet in trainset:\n",
    "        tweetFV = []\n",
    "        text = tweet['t']\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        tag = tweet['tags'][cid]\n",
    "\n",
    "        # Experiment\n",
    "        tagList.append(tag)\n",
    "\n",
    "        processedtext = processTweet(text)\n",
    "        tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "        featureList.extend(tweetFV)\n",
    "        tweetset.append((tweetFV,tag))\n",
    "\n",
    "    print(len(featureList))\n",
    "    featureListSet = set(featureList)\n",
    "    print(len(featureListSet))\n",
    "    featureList = list(featureListSet)\n",
    "\n",
    "    print('Got tweetset')\n",
    "    result = getSVMFeatureVectorAndLabels(tweetset, featureList)\n",
    "    print('Got FVs')\n",
    "    clf = svm.SVC(kernel = 'linear')\n",
    "    feature_vector = result['feature_vector']\n",
    "    labels = result['labels']\n",
    "    clf.fit(feature_vector, labels)\n",
    "    print('Trained')\n",
    "    \n",
    "\n",
    "# acc = 0\n",
    "# finacc = 0\n",
    "\n",
    "    for tweet in testset:\n",
    "        fv = []\n",
    "        # Test the classifier\n",
    "        testTweet = tweet['t']\n",
    "        processedTestTweet = processTweet(testTweet)\n",
    "        fv = getFeatureVector(processedTestTweet,fv)\n",
    "        res = getSVMFeatureVector(fv, featureList)\n",
    "        if (res == []):\n",
    "            continue\n",
    "        x = clf.predict(res)\n",
    "\n",
    "    #         fp.write(testTweet)\n",
    "    #         fp.write('\\t')\n",
    "    #         fp.write(str(x))\n",
    "    #         fp.write('\\t')\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "    #         fp.write(str(tweet['tags'][cid]))\n",
    "    #         fp.write('\\n')\n",
    "        if (x[0] == tweet['tags'][cid]):\n",
    "            acc = acc + 1\n",
    "\n",
    "    # fp.close()\n",
    "    print(acc)\n",
    "    acc = acc/len(testset)\n",
    "    print(acc)\n",
    "    finacc = finacc + acc\n",
    "\n",
    "finacc = finacc/n\n",
    "print(finacc)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tm = {}\n",
    "tm['t1'] = 0\n",
    "tm['t2'] = 1\n",
    "tm['t3'] = 4\n",
    "val = list(tm.values())\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = testset[2]\n",
    "text = tweet['t']\n",
    "# text = \"46 people alive \\m/\"\n",
    "print(text)\n",
    "ptt = processTweet(text)\n",
    "print(ptt)\n",
    "words = ptt.split()\n",
    "print(words)\n",
    "for w in words:\n",
    "    u = w.encode('unicode-escape')\n",
    "    print(u)\n",
    "        #replace two or more with two occurrences\n",
    "    w = replaceTwoOrMore(w)\n",
    "    print(w)\n",
    "        #strip punctuation\n",
    "    w = w.strip('\\'\"?,*.')\n",
    "    print(w)    \n",
    "#       check if the word starts with an alphabet\n",
    "    val = re.search(r\"^[a-zA-Z]\", w)\n",
    "    print(val)\n",
    "    val = re.search(b'\\\\U', u)\n",
    "    print(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training & n-fold Cross Validation of Max Entropy classifier\n",
    "\n",
    "n = 5\n",
    "dsize = len(cmntags)\n",
    "finacc = 0\n",
    "fp = open('featureList.txt','w')\n",
    "\n",
    "for i in range(0,1):\n",
    "    testset = []\n",
    "    trainset = cmntags\n",
    "#     ind1 = int(i*dsize/n)\n",
    "#     ind2 = int((i+1)*dsize/n)\n",
    "#     testset = cmntags[ind1:ind2]\n",
    "#     trainset = cmntags[:ind1]\n",
    "#     trainset.extend(cmntags[ind2:])\n",
    "    \n",
    "    featureList = []\n",
    "    tweetset = []\n",
    "    \n",
    "    #Exp\n",
    "    tagList = []\n",
    "    \n",
    "#    trainset = cmntags[0:5999]\n",
    "    for tweet in trainset:\n",
    "        tweetFV = []\n",
    "        text = tweet['t']\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        tag = tweet['tags'][cid]\n",
    "        \n",
    "        # Experiment\n",
    "        tagList.append(tag)\n",
    "        \n",
    "        processedtext = processTweet(text)\n",
    "        tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "        fp.write(' '.join(tweetFV))\n",
    "        featureList.extend(tweetFV)\n",
    "        tweetset.append((tweetFV,tag))\n",
    "\n",
    "    print(len(featureList))\n",
    "    featureListSet = set(featureList)\n",
    "    print(len(featureListSet))\n",
    "    featureList = list(featureListSet)\n",
    "    \n",
    "    tagListSet = set(tagList)\n",
    "    tagList = list(tagListSet)\n",
    "    print(tagList)\n",
    "\n",
    "    # Extract feature vector for all tweets in one shote\n",
    "    training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "    # Train the classifier\n",
    "    MaxEntClassifier = nltk.classify.maxent.MaxentClassifier.train(training_set, 'GIS', trace=3, \\\n",
    "    encoding=None, labels=None, gaussian_prior_sigma=0, max_iter = 5)\n",
    "    print('Training done')\n",
    "    \n",
    "    acc = 0\n",
    "    for tweet in testset:\n",
    "        fv = []\n",
    "        # Test the classifier\n",
    "        testTweet = tweet['t']\n",
    "        processedTestTweet = processTweet(testTweet)\n",
    "        x = MaxEntClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "#         fp.write(testTweet)\n",
    "#         fp.write('\\t')\n",
    "#         fp.write(str(x))\n",
    "#         fp.write('\\t')\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "#         fp.write(str(tweet['tags'][cid]))\n",
    "#         fp.write('\\n')\n",
    "        if (x == tweet['tags'][cid]):\n",
    "            acc = acc + 1\n",
    "\n",
    "# fp.close()\n",
    "    print(acc)\n",
    "    acc = acc/len(testset)\n",
    "    print(acc)\n",
    "    finacc = finacc + acc\n",
    "\n",
    "finacc = finacc/n\n",
    "print(finacc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in data:\n",
    "        tweet = cmntags[i]\n",
    "        cid1 = tweet['edits'][0]['curator_id']\n",
    "        cid2 = tweet['edits'][0]['curator_id']\n",
    "        tag1 = tweet['tags'][cid1]\n",
    "        tag2 = tweet['tags'][cid1]\n",
    "        if (tag1 == 0)|(tag2 == 0):\n",
    "            pprint(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "t = \"blah #ToKi9llAMockingbird #onemore blah\"\n",
    "# t1 = re.match(r'#([^\\s])', t)\n",
    "# t = \"#A\"\n",
    "regex = re.compile(r'#([^\\s]+)')\n",
    "matchObj = regex.findall(t)\n",
    "print(matchObj)\n",
    "s = len(matchObj)\n",
    "fv = []\n",
    "for i in range(0,s):\n",
    "    word = matchObj[i]\n",
    "    startInd = len(word)\n",
    "    stopInd = 0\n",
    "    for i in range(0,len(word)):\n",
    "        if (i==(len(word)-1)):\n",
    "                    stopInd = i + 1\n",
    "                    if (startInd == len(word)):\n",
    "                        startInd = 0\n",
    "                    fv.append(word[startInd:stopInd])\n",
    "                    continue\n",
    "        if (word[i].isupper()):\n",
    "            if (startInd != len(word)):\n",
    "                stopInd = i\n",
    "                print(i)\n",
    "                print(word[startInd:stopInd])\n",
    "                fv.append(word[startInd:stopInd])\n",
    "                startInd = i\n",
    "            else:\n",
    "                if (i != 0):\n",
    "                    startInd = 0\n",
    "                    stopInd = i\n",
    "                    fv.append(word[startInd:stopInd])\n",
    "                    startInd = i\n",
    "                \n",
    "print(fv)\n",
    "        \n",
    "        \n",
    "# matchObj = re.sub(r'#([^\\s]+)',r'\\1',matchObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"hhaahahhahahaa\"\n",
    "pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "s = pattern.sub(r\"\\1\\1\", s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "testset = difftags\n",
    "# trainset = []\n",
    "# ind1 = int(i*dsize/n)\n",
    "# ind2 = int((i+1)*dsize/n)\n",
    "# testset = cmntags[ind1:ind2]\n",
    "trainset = cmntags\n",
    "# trainset.extend(cmntags[ind2:])\n",
    "\n",
    "featureList = []\n",
    "tweetset = []\n",
    "\n",
    "#Exp\n",
    "tagList = []\n",
    "\n",
    "#    trainset = cmntags[0:5999]\n",
    "for tweet in trainset:\n",
    "    tweetFV = []\n",
    "    text = tweet['t']\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    tag = tweet['tags'][cid]\n",
    "\n",
    "    # Experiment\n",
    "    tagList.append(tag)\n",
    "\n",
    "    processedtext = processTweet(text)\n",
    "    tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "    featureList.extend(tweetFV)\n",
    "    for i in range(0,len(tweetID)):\n",
    "        if (tweetID[i] == tweet['_id']):\n",
    "            tweetFV.extend(region[i])\n",
    "            tweetFV.extend(state[i])\n",
    "            tweetFV.extend(county[i])\n",
    "            tweetFV.extend(mregion[i])\n",
    "            break\n",
    "\n",
    "    tweetset.append((tweetFV,tag))\n",
    "\n",
    "featureList.extend(list(regionSet))\n",
    "featureList.extend(list(stateSet))\n",
    "featureList.extend(list(countySet))\n",
    "featureList.extend(list(mregionSet))\n",
    "featureList.extend(list(stateSet))\n",
    "print(len(featureList))\n",
    "featureListSet = set(featureList)\n",
    "print(len(featureListSet))\n",
    "featureList = list(featureListSet)\n",
    "#     flist.write(\"\\n\".join(featureList))\n",
    "\n",
    "tagListSet = set(tagList)\n",
    "tagList = list(tagListSet)\n",
    "print(tagList)\n",
    "print('FVs done')\n",
    "\n",
    "# Extract feature vector for all tweets in one shote\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "# Train the classifier\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "fn = 0\n",
    "fp = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "for tweet in testset:\n",
    "    fv = []\n",
    "    # Test the classifier\n",
    "    testTweet = tweet['t']\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    tweet['ctags'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = difftags\n",
    "for tweet in testset:\n",
    "    fv = []\n",
    "    # Test the classifier\n",
    "    testTweet = tweet['t']\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    tweet['ctags'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count1 = 0\n",
    "count3 = 0\n",
    "for tweet in testset:\n",
    "        if (tweet['ctags'] == 1):\n",
    "            count1 = count1 + 1\n",
    "        if (tweet['ctags'] == 3):\n",
    "            count3 = count3 + 1\n",
    "print(count1)\n",
    "print(count3)\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(count1)\n",
    "print(count3)\n",
    "print(len(testset))\n",
    "cache_testdata = testset\n",
    "print(len(cache_testdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('tweet_master_data.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# hello = [['Me','You'],['293', '219'],['13','15']]\n",
    "hello = testset[0:3]\n",
    "length = len(hello[0])\n",
    "\n",
    "with open('test1.csv', 'w') as testfile:\n",
    "    csv_writer = csv.writer(testfile)\n",
    "#     for y in range(length):\n",
    "    csv_writer.writerow([x['_id'] for x in hello])\n",
    "    csv_writer.writerow([x['cc'] for x in hello])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "fp = open('test.csv', 'w', newline='')\n",
    "a = csv.writer(fp, delimiter=',')\n",
    "a.writerow(('Time Stamp', 'Tweet Longitude', 'Tweet Latitude', 'Country', 'Region', 'State', 'County', 'Microregion'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = []\n",
    "with open('tweet_master_data.json') as f:\n",
    "    for line in f:\n",
    "        data2.extend(json.loads(line))\n",
    "        \n",
    "dsize = len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count3cmn = 0\n",
    "count3diff = 0\n",
    "count2cmn = 0\n",
    "countcmn = 0\n",
    "for i in range(0,len(data2)):\n",
    "    tweet = data2[i]\n",
    "    if (tweet['lang'] == 'es')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for j in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][j]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        countcmn = countcmn + 1\n",
    "        if (tweet['tags'][cid1] == 3):\n",
    "            count3cmn = count3cmn + 1\n",
    "        if (tweet['tags'][cid1] == 2):\n",
    "            count2cmn = count2cmn + 1\n",
    "#     else:\n",
    "#         if(tweet['ctags'] == 3):\n",
    "#             count3diff = count3diff + 1\n",
    "print(count3cmn)\n",
    "print(countcmn)\n",
    "print(count2cmn)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        \n",
    "for i in range(0,dsize):\n",
    "    tweet = data2[i]\n",
    "    if (tweet['lang'] == 'es')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for j in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][j]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        continue\n",
    "#         print('Common Tags')\n",
    "#         if (tweet['tags'][cid1] == 3):\n",
    "#             print('Sickness Tweet')\n",
    "#             a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))\n",
    "    else:\n",
    "#         print('Different Tags')\n",
    "        testTweet = tweet['t']\n",
    "        processedTestTweet = processTweet(testTweet)\n",
    "        x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "        data2[i].update({'ctags':x})\n",
    "#         print('Classified')\n",
    "#         if (x == 3):\n",
    "#             print('Sickness Tweet')\n",
    "#             a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))       \n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint(data2[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count3cmn = 0\n",
    "count3diff = 0\n",
    "for i in range(0,len(data2)):\n",
    "    tweet = data2[i]\n",
    "    if (tweet['lang'] == 'es')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for j in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][j]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        if (tweet['tags'][cid1] == 3):\n",
    "            count3cmn = count3cmn + 1\n",
    "    else:\n",
    "        if(tweet['ctags'] == 3):\n",
    "            count3diff = count3diff + 1\n",
    "print(count3cmn)\n",
    "print(count3diff)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint(data2[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
