{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Python's json Encoder and Decoder\n",
    "import json\n",
    "from pprint import pprint # Pretty Print\n",
    "\n",
    "# Parsing of json file as one list\n",
    "# Name of file which combines location information with validated dengue tweets: 'tweet_master_data.json'\n",
    "data = []\n",
    "with open('tweet_master_data.json') as jdata:\n",
    "    data = json.load(jdata)\n",
    "    jdata.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a tweet data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a tweet data point\n",
      "{'_id': '299714679532556288',\n",
      " 'cc': 'BR',\n",
      " 'country': 'Brasil',\n",
      " 'county': 'RondonÃ³polis',\n",
      " 'cr': '2013-02-07T22:02:02',\n",
      " 'edits': [{'curator_id': '540e300a7673826b17a5604c',\n",
      "            'date': '2015-07-06T01:18:27.900000',\n",
      "            'field': 'tags',\n",
      "            'new': 1},\n",
      "           {'curator_id': '555232868624c82a1c6d2ca3',\n",
      "            'date': '2015-07-06T17:40:18.407000',\n",
      "            'field': 'tags',\n",
      "            'new': 1,\n",
      "            'old': 1}],\n",
      " 'f': 'tw2013272123',\n",
      " 'lang': 'pt',\n",
      " 'loc': ' MT / PR',\n",
      " 'microregion': 'MicrorregiÃ£o de RondonÃ³polis',\n",
      " 'p': '48401b8f7232dfb8',\n",
      " 'pln': -54.607,\n",
      " 'plt': -16.572,\n",
      " 'region': 'RegiÃ£o Centro-Oeste',\n",
      " 'state': 'MT',\n",
      " 't': 'Dengue ðŸ˜«',\n",
      " 'tags': {'540e300a7673826b17a5604c': 1, '555232868624c82a1c6d2ca3': 1},\n",
      " 'tln': -54.649,\n",
      " 'tlt': -16.463,\n",
      " 'uid': '419780633',\n",
      " 'v': True}\n"
     ]
    }
   ],
   "source": [
    "print('Example of a tweet data point')\n",
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tweet comprises of several fields. Tweets are stored as a list of dictionaries in the json file \n",
    "\n",
    "### Keys in json file\n",
    "* 'v' : validated (true/false)\n",
    "* 'tags' : dictionary with key (curator ID) and value (label).  The labels are as follows: 1=junk, 2=report, 3=sickness\n",
    "* 'edits' : a dictionary that keeps track of all tags applied by curators.  Not really useful unless you want to see if somebody is re-rating tweets or if they erase tags by accident.  \n",
    "* \"_id\" : tweet ID (also the object ID for the mongo db)\n",
    "* \"lang\" : language of tweet\n",
    "* \"loc\" : user-entered location name\n",
    "* \"plt\" : profile latitude coordinates\n",
    "* \"pln\" : profile longitude\n",
    "* \"uid\" : twitter user id\n",
    "* \"tlt\" : tweet latitude\n",
    "* \"tln\" : tweet longitude\n",
    "* \"cc\" : country code\n",
    "* \"f\" : our own backup coding\n",
    "* \"p\" : twitter place ID (not sure if these can be looked up somehow via twitter) \n",
    "* \"t\" : tweet text\n",
    "* \"acr\": time of the userâ€™s account creation in UTC\n",
    "* \"cr\" : time of the tweet in UTC \n",
    "* \"flrs\": number of followers\n",
    "* \"flng\" : number of accounts following (friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points (tweets) = 13513\n",
      "Number of portuguese tweets = 10116\n",
      "Number of tweets tagged as 'sick' or 'junk' only = 9965\n",
      "Number of tweets with common tags by annotators = 7758\n",
      "Number of tweets with different tags by annotators = 2207\n"
     ]
    }
   ],
   "source": [
    "# Check the size of dataset\n",
    "print(\"Number of data points (tweets) = \", end = '')\n",
    "print(len(data))\n",
    "\n",
    "# EXtract portuguese tweets only from the dataset, since we are performing this analysis for Brazil\n",
    "pt_tweets = []\n",
    "for tweet in data:\n",
    "    # Portuguese Tweets are encoded as 'pt' and spanish tweets are encoded as 'es' under the key 'lang'\n",
    "    if tweet['lang'] == 'pt':\n",
    "        pt_tweets.append(tweet)\n",
    "\n",
    "print(\"Number of portuguese tweets = \", end = '')\n",
    "print(len(pt_tweets))\n",
    "# Release memory of redundant variables\n",
    "del data\n",
    "\n",
    "# The three types of tags used in manual classification of the tweets are 'report', 'junk' and 'sickness'\n",
    "# Extraction of tweets tagged as junk(1) or sickness(3) only\n",
    "pt13_tweets = []\n",
    "for tweet in pt_tweets:\n",
    "    # Identify curator ids of each tweet to refer their tags\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for i in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][i]['curator_id'])\n",
    "    cidListSet = set(cidList) # Eliminates redundancy in set elements\n",
    "    cidList = list(cidListSet) \n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    if (tweet['tags'][cid1] != 2 and tweet['tags'][cid2] != 2):\n",
    "        pt13_tweets.append(tweet)\n",
    "\n",
    "# Check the size of dataset containing portuguese tweets tagged as 'junk' or 'sickness' only\n",
    "print(\"Number of tweets tagged as 'sick' or 'junk' only = \", end = '')\n",
    "print(len(pt13_tweets))\n",
    "# Release memory of redundant variables\n",
    "del pt_tweets\n",
    "\n",
    "# Differentiating tweets into those with common tags and those with different tags - easier for manipulation\n",
    "\n",
    "cmntags = [] # For tweets with annotators' agreement\n",
    "difftags = [] # For tweets with annotators' diasgreement\n",
    "zerotag = []\n",
    "for tweet in pt13_tweets:\n",
    "    editlen = len(tweet['edits'])\n",
    "    cidList = []\n",
    "    for i in range(0,editlen):\n",
    "        cidList.append(tweet['edits'][i]['curator_id'])\n",
    "    cidListSet = set(cidList)\n",
    "    cidList = list(cidListSet)\n",
    "    cid1 = cidList[0]\n",
    "    cid2 = cidList[1]\n",
    "    # Curators do not agree on the annotation of all tweets - clash of tags\n",
    "    if (tweet['tags'][cid1] == tweet['tags'][cid2]):\n",
    "        cmntags.append(tweet)\n",
    "    else:\n",
    "        difftags.append(tweet)\n",
    "    \n",
    "#     # Tweets tagged as (0) - System Error\n",
    "#     if ((tweet['tags'][cid1] == 0)|(tweet['tags'][cid2] == 0)):\n",
    "#         zerotag.append(tweet)\n",
    "\n",
    "print(\"Number of tweets with common tags by annotators = \", end = '')\n",
    "print(len(cmntags)) # Check size of dataset containing tweets annotated with common tags\n",
    "print(\"Number of tweets with different tags by annotators = \", end = '')\n",
    "print(len(difftags)) # Check size of dataset containing tweets annotated with common tags\n",
    "del pt13_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only those tweets with annotators' agreement are considered for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'junk' tweets in training set: 5261\n",
      "Number of 'sickness' tweets in training set: 2497\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of tweets tagged as junk and sickness \n",
    "count3 = 0\n",
    "for tweet in cmntags:\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    if(tweet['tags'][cid] == 3):\n",
    "        count3 = count3 + 1\n",
    "# end\n",
    "print(\"Number of 'junk' tweets in training set:\", end = \" \")\n",
    "print(len(cmntags)-count3)\n",
    "print(\"Number of 'sickness' tweets in training set:\", end = \" \")\n",
    "print(count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Function for pre-processing of tweets\n",
    "def processTweet(tweet):\n",
    "\n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    # Convert hyperlinks to a generic term 'URL' or an empty space\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?:\\/\\/[^\\s]+))','URL',tweet)\n",
    "#     tweet = re.sub('((www\\.[^\\s]+)|(https?:\\/\\/[^\\s]+))','',tweet)\n",
    "    # Convert @username to USER or an empty space\n",
    "    tweet = re.sub('(@[^\\s]+)|(@[\\s][^\\s]+)','USER',tweet)\n",
    "#     tweet = re.sub('(@[^\\s]+)|(@[\\s][^\\s]+)','',tweet)\n",
    "    # Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    # Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    # Trim special charaters\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import NLTK library and portuguese components for manipulation of tweets\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "# print(stopwords[0:10])\n",
    "stopwords.append('USER')\n",
    "stopwords.append('URL')\n",
    "\n",
    "# Function to replace colloquial social meda words containing repeated character with a single instance of the character\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.*)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "# Function to prepare the classifier feature vector from raw data\n",
    "def getFeatureVector(tweet, featureVector):\n",
    "    # split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        w = re.sub(r'#([^\\s]+)', r'\\1', w)\n",
    "        # replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        # strip punctuation\n",
    "        w = w.strip('\\'\"?,*.(_!)/')\n",
    "        w = w.replace('\\\\','')\n",
    "        w = w.replace('/','')\n",
    "        \n",
    "        # check if the word starts with an alphabet\n",
    "        # val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        val = re.search(r\"^[a-zA-Z]\", w)\n",
    "\n",
    "        # check if the word contains only numbers\n",
    "        valnum = re.search(r\"^[0-9][0-9]*$\", w)\n",
    "    \n",
    "        # Addition of emoticons to feature list - Commment the next paragraph to omit emojis\n",
    "        u = w.encode('unicode-escape')\n",
    "        bval = re.search(b'\\\\U', u)\n",
    "        if (bval):\n",
    "            s = u.split(b'\\\\U')\n",
    "            for l in range(1,len(s)):\n",
    "                a = (b'\\\\U' + s[l])\n",
    "                astr = a.decode('unicode-escape')\n",
    "                featureVector.append(astr)\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        # ignore if it is a stop word or the word contains only numbers or the word does not start with an alphabet\n",
    "        if (w in stopwords or (val is None) or valnum):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "            \n",
    "    featureVectorSet = set(featureVector) # Eliminate redundant features\n",
    "    featureVector = list(featureVectorSet) \n",
    "    del featureVectorSet\n",
    "    \n",
    "    # Additional code to use bigrams and trigrams; these features have not been used for results reported in the paper\n",
    "    # Addition of Bigrams as features        \n",
    "#     for j in range(0,len(featureVector)-1):\n",
    "#         featureVector.append((featureVector[j],featureVector[j+1]))\n",
    "        \n",
    "    # Addition of Trigrams as features        \n",
    "#     for j in range(0,len(featureVector)-2):\n",
    "#         featureVector.append((featureVector[j],featureVector[j+1],featureVector[j+2]))\n",
    "    \n",
    "    # Break down a hashtag into individual words whenever possible to get additional cues about topic of the tweet\n",
    "    # Example: Hashtag1 - #ifyouknowwhatimean, Hashtag2 - #IfYouKnowWhatIMean\n",
    "    # Hashtag1 can not be analysed further without a dictionary, but Hashtag2 can be fragmented at the capital letters            \n",
    "    regex = re.compile(r'#([^\\s]+)')\n",
    "    matchObj = regex.findall(tweet)\n",
    "#     print(matchObj)\n",
    "    s = len(matchObj) # Multiple hashtags\n",
    "    fv = []\n",
    "    for i in range(0,s):\n",
    "        word = matchObj[i]\n",
    "        # Initialization\n",
    "        startInd = len(word)\n",
    "        stopInd = 0\n",
    "        for i in range(0,len(word)):\n",
    "            if (i==(len(word)-1)):\n",
    "                        stopInd = i + 1\n",
    "                        if (startInd == len(word)):\n",
    "                            startInd = 0\n",
    "                        # Single capital letter identified (Example: 'I')\n",
    "                        fv.append(word[startInd:stopInd].lower()) # Single Capital Letter identified at the end of tag\n",
    "                        continue\n",
    "            if (word[i].isupper()):\n",
    "                if (startInd != len(word)):\n",
    "                    stopInd = i\n",
    "                    # Word identified within the phrase\n",
    "                    fv.append(word[startInd:stopInd].lower()) # \n",
    "                    startInd = i\n",
    "                else:\n",
    "                    if (i != 0):\n",
    "                        startInd = 0\n",
    "                        stopInd = i\n",
    "                        # Word identified at the starting of the phrase\n",
    "                        fv.append(word[startInd:stopInd].lower())\n",
    "                        startInd = i\n",
    "    \n",
    "    featureVector.extend(fv)\n",
    "    return featureVector\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start extract_features\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        if (type(word) is tuple):\n",
    "            temp = 'contains' + str(word)\n",
    "            features[temp] = (word in tweet_words)\n",
    "            del temp\n",
    "        else:\n",
    "            features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44173\n",
      "7738\n",
      "Preparation of Feature Vectors completed\n",
      "Round 0 in cross validation\n",
      "Accuracy: 0.8175370728562218\n",
      "Precision: 0.7019748653500898\n",
      "Recall: 0.7696850393700787\n",
      "TNR: 0.840843720038351\n",
      "\n",
      "\n",
      "Most Informative Features\n",
      "        contains(parado) = True                1 : 3      =    165.9 : 1.0\n",
      "         contains(ebola) = True                1 : 3      =     35.1 : 1.0\n",
      "      contains(mosquito) = True                1 : 3      =     30.2 : 1.0\n",
      "         contains(dando) = True                1 : 3      =     19.2 : 1.0\n",
      "          contains(agua) = True                1 : 3      =     17.9 : 1.0\n",
      "         contains(saÃºde) = True                1 : 3      =     15.9 : 1.0\n",
      "         contains(irmÃ£o) = True                3 : 1      =     15.4 : 1.0\n",
      "      contains(hospital) = True                3 : 1      =     15.3 : 1.0\n",
      "      contains(gusttavo) = True                3 : 1      =     14.8 : 1.0\n",
      "          contains(acha) = True                3 : 1      =     14.1 : 1.0\n",
      "       contains(cuidado) = True                1 : 3      =     13.7 : 1.0\n",
      "       contains(consigo) = True                3 : 1      =     13.4 : 1.0\n",
      "          contains(irmÃ£) = True                3 : 1      =     13.1 : 1.0\n",
      "           contains(rua) = True                1 : 3      =     12.7 : 1.0\n",
      "          contains(lima) = True                3 : 1      =     12.3 : 1.0\n",
      "       contains(tadinha) = True                3 : 1      =     12.3 : 1.0\n",
      "           contains(tÃ£o) = True                1 : 3      =     12.2 : 1.0\n",
      "           contains(pai) = True                3 : 1      =     12.2 : 1.0\n",
      "          contains(mata) = True                1 : 3      =     12.1 : 1.0\n",
      "             contains(ðŸ’‰) = True                3 : 1      =     12.0 : 1.0\n",
      "None\n",
      "46472\n",
      "8332\n",
      "Preparation of Feature Vectors completed\n",
      "Round 1 in cross validation\n",
      "Accuracy: 0.8692010309278351\n",
      "Precision: 0.7190775681341719\n",
      "Recall: 0.8325242718446602\n",
      "TNR: 0.8824561403508772\n",
      "\n",
      "\n",
      "Most Informative Features\n",
      "        contains(parado) = True                1 : 3      =    142.5 : 1.0\n",
      "      contains(mosquito) = True                1 : 3      =     37.5 : 1.0\n",
      "         contains(ebola) = True                1 : 3      =     30.9 : 1.0\n",
      "         contains(dores) = True                3 : 1      =     24.4 : 1.0\n",
      "       contains(tadinha) = True                3 : 1      =     17.8 : 1.0\n",
      "         contains(dando) = True                1 : 3      =     17.7 : 1.0\n",
      "          contains(agua) = True                1 : 3      =     17.4 : 1.0\n",
      "         contains(irmÃ£o) = True                3 : 1      =     16.7 : 1.0\n",
      "          contains(irmÃ£) = True                3 : 1      =     15.4 : 1.0\n",
      "           contains(tÃ£o) = True                1 : 3      =     15.2 : 1.0\n",
      "             contains(ðŸ’‰) = True                3 : 1      =     15.1 : 1.0\n",
      "          contains(copa) = True                1 : 3      =     15.0 : 1.0\n",
      "        contains(veneno) = True                1 : 3      =     13.7 : 1.0\n",
      "            contains(tl) = True                1 : 3      =     11.6 : 1.0\n",
      "         contains(saÃºde) = True                1 : 3      =     11.6 : 1.0\n",
      "         contains(surto) = True                1 : 3      =     11.3 : 1.0\n",
      "          contains(acho) = True                3 : 1      =     11.2 : 1.0\n",
      "          contains(vira) = True                1 : 3      =     11.0 : 1.0\n",
      "             contains(ðŸ˜­) = True                3 : 1      =     10.9 : 1.0\n",
      "            contains(vÃ³) = True                3 : 1      =     10.7 : 1.0\n",
      "None\n",
      "46595\n",
      "8251\n",
      "Preparation of Feature Vectors completed\n",
      "Round 2 in cross validation\n",
      "Accuracy: 0.842682140554481\n",
      "Precision: 0.7541528239202658\n",
      "Recall: 0.8254545454545454\n",
      "TNR: 0.8521478521478522\n",
      "\n",
      "\n",
      "Most Informative Features\n",
      "        contains(parado) = True                1 : 3      =    156.2 : 1.0\n",
      "      contains(mosquito) = True                1 : 3      =     27.6 : 1.0\n",
      "          contains(agua) = True                1 : 3      =     26.1 : 1.0\n",
      "         contains(dores) = True                3 : 1      =     25.5 : 1.0\n",
      "         contains(irmÃ£o) = True                3 : 1      =     21.4 : 1.0\n",
      "          contains(lima) = True                3 : 1      =     21.1 : 1.0\n",
      "         contains(dando) = True                1 : 3      =     16.9 : 1.0\n",
      "       contains(segunda) = True                3 : 1      =     15.3 : 1.0\n",
      "     contains(professor) = True                3 : 1      =     13.9 : 1.0\n",
      "           contains(dÃ³i) = True                3 : 1      =     12.4 : 1.0\n",
      "           contains(mae) = True                3 : 1      =     12.3 : 1.0\n",
      "             contains(ðŸ˜¥) = True                3 : 1      =     12.2 : 1.0\n",
      "        contains(veneno) = True                1 : 3      =     12.0 : 1.0\n",
      "       contains(imagina) = True                1 : 3      =     12.0 : 1.0\n",
      "          contains(devo) = True                3 : 1      =     11.8 : 1.0\n",
      "             contains(ðŸ˜­) = True                3 : 1      =     11.3 : 1.0\n",
      "       contains(gustavo) = True                3 : 1      =     10.9 : 1.0\n",
      "         contains(sinto) = True                3 : 1      =     10.9 : 1.0\n",
      "           contains(tÃ£o) = True                1 : 3      =     10.9 : 1.0\n",
      "      contains(hospital) = True                3 : 1      =     10.5 : 1.0\n",
      "None\n",
      "44495\n",
      "7731\n",
      "Preparation of Feature Vectors completed\n",
      "Round 3 in cross validation\n",
      "Accuracy: 0.8402061855670103\n",
      "Precision: 0.8134920634920635\n",
      "Recall: 0.7269503546099291\n",
      "TNR: 0.9048582995951417\n",
      "\n",
      "\n",
      "Most Informative Features\n",
      "          contains(agua) = True                1 : 3      =     43.3 : 1.0\n",
      "         contains(ebola) = True                1 : 3      =     33.3 : 1.0\n",
      "      contains(mosquito) = True                1 : 3      =     28.8 : 1.0\n",
      "       contains(tadinha) = True                3 : 1      =     21.4 : 1.0\n",
      "             contains(ðŸ’‰) = True                3 : 1      =     18.4 : 1.0\n",
      "         contains(irmÃ£o) = True                3 : 1      =     18.0 : 1.0\n",
      "          contains(irmÃ£) = True                3 : 1      =     17.2 : 1.0\n",
      "      contains(gusttavo) = True                3 : 1      =     16.9 : 1.0\n",
      "         contains(dores) = True                3 : 1      =     15.5 : 1.0\n",
      "     contains(professor) = True                3 : 1      =     14.0 : 1.0\n",
      "           contains(rua) = True                1 : 3      =     13.7 : 1.0\n",
      "         contains(saÃºde) = True                1 : 3      =     13.7 : 1.0\n",
      "          contains(copa) = True                1 : 3      =     13.4 : 1.0\n",
      "           contains(tÃ£o) = True                1 : 3      =     13.4 : 1.0\n",
      "      contains(hospital) = True                3 : 1      =     12.9 : 1.0\n",
      "       contains(cuidado) = True                1 : 3      =     12.8 : 1.0\n",
      "            contains(tl) = True                1 : 3      =     12.8 : 1.0\n",
      "          contains(prof) = True                3 : 1      =     12.5 : 1.0\n",
      "       contains(imagina) = True                1 : 3      =     12.2 : 1.0\n",
      "          contains(lima) = True                3 : 1      =     11.9 : 1.0\n",
      "None\n",
      "47121\n",
      "8478\n",
      "Preparation of Feature Vectors completed\n",
      "Round 4 in cross validation\n",
      "Accuracy: 0.8917525773195877\n",
      "Precision: 0.8053830227743272\n",
      "Recall: 0.8401727861771058\n",
      "TNR: 0.9136822773186409\n",
      "\n",
      "\n",
      "Most Informative Features\n",
      "        contains(parado) = True                1 : 3      =    133.5 : 1.0\n",
      "         contains(ebola) = True                1 : 3      =     33.0 : 1.0\n",
      "      contains(mosquito) = True                1 : 3      =     31.6 : 1.0\n",
      "         contains(irmÃ£o) = True                3 : 1      =     30.8 : 1.0\n",
      "         contains(dores) = True                3 : 1      =     25.3 : 1.0\n",
      "             contains(ðŸ’‰) = True                3 : 1      =     17.1 : 1.0\n",
      "        contains(amanha) = True                3 : 1      =     17.1 : 1.0\n",
      "       contains(segunda) = True                3 : 1      =     17.1 : 1.0\n",
      "          contains(agua) = True                1 : 3      =     16.7 : 1.0\n",
      "          contains(irmÃ£) = True                3 : 1      =     16.0 : 1.0\n",
      "          contains(copa) = True                1 : 3      =     14.1 : 1.0\n",
      "         contains(dando) = True                1 : 3      =     13.6 : 1.0\n",
      "          contains(lima) = True                3 : 1      =     13.0 : 1.0\n",
      "        contains(veneno) = True                1 : 3      =     12.2 : 1.0\n",
      "       contains(repouso) = True                3 : 1      =     11.6 : 1.0\n",
      "           contains(dÃ³i) = True                3 : 1      =     11.6 : 1.0\n",
      "         contains(saÃºde) = True                1 : 3      =     11.6 : 1.0\n",
      "             contains(ðŸ˜­) = True                3 : 1      =     11.1 : 1.0\n",
      "       contains(tadinha) = True                3 : 1      =     11.1 : 1.0\n",
      "            contains(vÃ³) = True                3 : 1      =     11.1 : 1.0\n",
      "None\n",
      "Final Accuracy: 0.8522758014450271\n",
      "Final Precision: 0.7588160687341837\n",
      "Final Recall: 0.7989573994912638\n",
      "Final True Negative Rate: 0.8787976578901725\n"
     ]
    }
   ],
   "source": [
    "# Training & n-fold Cross Validation of Naive Bayes classifier\n",
    "n = 5\n",
    "\n",
    "count = 0\n",
    "dsize = len(cmntags)\n",
    "finacc = 0\n",
    "finprec = 0\n",
    "fintnr = 0\n",
    "finrecall = 0\n",
    "# flist = open('featureList.txt','w')\n",
    "\n",
    "for i in range(0,n):\n",
    "    testset = []\n",
    "    trainset = []\n",
    "    ind1 = int(i*dsize/n)\n",
    "    ind2 = int((i+1)*dsize/n)\n",
    "    testset = cmntags[ind1:ind2]\n",
    "    trainset = cmntags[:ind1]\n",
    "    trainset.extend(cmntags[ind2:])\n",
    "    \n",
    "    featureList = []\n",
    "    tweetset = []\n",
    "    \n",
    "    #Exp\n",
    "    tagList = []\n",
    "    \n",
    "#    trainset = cmntags[0:5999]\n",
    "    for tweet in trainset:\n",
    "        tweetFV = []\n",
    "        text = tweet['t']\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        tag = tweet['tags'][cid]\n",
    "        \n",
    "        processedtext = processTweet(text)\n",
    "        tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "        \n",
    "        # Addition of location data to feature vector - Comment the next 4 lines to omit location data from features\n",
    "#         tweetFV.append(tweet['region'])\n",
    "#         tweetFV.append(tweet['state'])\n",
    "#         tweetFV.append(tweet['county'])\n",
    "#         tweetFV.append(tweet['microregion'])\n",
    "        \n",
    "        tweetFVSet = set(tweetFV)\n",
    "        tweetFV = list(tweetFVSet)\n",
    "        del tweetFVSet\n",
    "        featureList.extend(tweetFV)\n",
    "        tweetset.append((tweetFV,tag))\n",
    "\n",
    "    print(len(featureList))\n",
    "    featureListSet = set(featureList)\n",
    "    print(len(featureListSet))\n",
    "    featureList = list(featureListSet)\n",
    "    featureListStr = [str(item) for item in featureList]\n",
    "#     flist.write(\"\\t\".join(featureListStr))\n",
    "    \n",
    "    print('Preparation of Feature Vectors completed')\n",
    "\n",
    "    # Extract feature vector for all tweets in one shot\n",
    "    training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "    # Train the classifier\n",
    "    NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    \n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    sick_tn = 0\n",
    "    sick_tp = 0\n",
    "    sick_fn = 0\n",
    "    sick_tp = 0\n",
    "\n",
    "    for j in range(0,len(testset)):\n",
    "        tweet = testset[j]\n",
    "        fv = []\n",
    "        # Test the classifier\n",
    "        testTweet = tweet['t']\n",
    "        processedTestTweet = processTweet(testTweet)\n",
    "        x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "#         pt_tweets[j]['ctags'] = x\n",
    "        cid = tweet['edits'][0]['curator_id']\n",
    "        if (x == tweet['tags'][cid]):\n",
    "            if (x == 1):\n",
    "                tn += 1\n",
    "            else:\n",
    "                tp += 1\n",
    "        else:\n",
    "            if (x == 1):\n",
    "                fn += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "    acc = (tp + tn)/(tp + tn + fp + fn)\n",
    "    prec = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    tnr = tn/(tn + fp)\n",
    "    print(\"Round\", i, \"in cross validation\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"TNR:\", tnr)\n",
    "    print('\\n')\n",
    "    \n",
    "    finacc = finacc + acc\n",
    "    finrecall = finrecall + recall\n",
    "    finprec = finprec + prec\n",
    "    fintnr = fintnr + tnr\n",
    "    \n",
    "    print(NBClassifier.show_most_informative_features(20))\n",
    "\n",
    "finacc = finacc/n\n",
    "finrecall = finrecall/n\n",
    "finprec = finprec/n\n",
    "fintnr = fintnr/n\n",
    "print(\"Final Accuracy:\", finacc)\n",
    "print(\"Final Precision:\", finprec)\n",
    "print(\"Final Recall:\", finrecall)\n",
    "print(\"Final True Negative Rate:\", fintnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57214\n",
      "9368\n",
      "FVs done\n"
     ]
    }
   ],
   "source": [
    "# To print the most informative features using the entire dataset\n",
    "\n",
    "dsize = len(cmntags)\n",
    "flist = open('featureList.txt','w')\n",
    "\n",
    "trainset = cmntags\n",
    "\n",
    "featureList = []\n",
    "tweetset = []\n",
    "\n",
    "\n",
    "for tweet in trainset:\n",
    "    tweetFV = []\n",
    "    text = tweet['t']\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    tag = tweet['tags'][cid]\n",
    "\n",
    "    processedtext = processTweet(text)\n",
    "    tweetFV = getFeatureVector(processedtext, tweetFV)\n",
    "    \n",
    "    # Addition of location data to feature vectors - commment the next 4 lines to omit location data from features\n",
    "#     tweetFV.append(tweet['region'])\n",
    "#     tweetFV.append(tweet['state'])\n",
    "#     tweetFV.append(tweet['county'])\n",
    "#     tweetFV.append(tweet['microregion'])\n",
    "    \n",
    "    tweetFVSet = set(tweetFV)\n",
    "    tweetFV = list(tweetFVSet)\n",
    "    del tweetFVSet\n",
    "    featureList.extend(tweetFV)\n",
    "    tweetset.append((tweetFV,tag))\n",
    "\n",
    "print(len(featureList))\n",
    "featureListSet = set(featureList)\n",
    "print(len(featureListSet))\n",
    "featureList = list(featureListSet)\n",
    "featureListStr = [str(item) for item in featureList]\n",
    "flist.write(\"\\t\".join(featureListStr))\n",
    "\n",
    "print('FVs done')\n",
    "\n",
    "# Extract feature vector for all tweets in one shote\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweetset)\n",
    "\n",
    "# Train the classifier\n",
    "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        contains(parado) = True                1 : 3      =    189.7 : 1.0\n",
      "         contains(ebola) = True                1 : 3      =     42.3 : 1.0\n",
      "      contains(mosquito) = True                1 : 3      =     31.3 : 1.0\n",
      "         contains(dores) = True                3 : 1      =     28.8 : 1.0\n",
      "          contains(agua) = True                1 : 3      =     22.3 : 1.0\n",
      "         contains(dando) = True                1 : 3      =     21.6 : 1.0\n",
      "          contains(irmÃ£) = True                3 : 1      =     19.8 : 1.0\n",
      "         contains(irmÃ£o) = True                3 : 1      =     19.6 : 1.0\n",
      "             contains(ðŸ’‰) = True                3 : 1      =     19.0 : 1.0\n",
      "          contains(copa) = True                1 : 3      =     16.6 : 1.0\n",
      "      contains(gusttavo) = True                3 : 1      =     16.1 : 1.0\n",
      "        contains(veneno) = True                1 : 3      =     15.0 : 1.0\n",
      "     contains(professor) = True                3 : 1      =     14.7 : 1.0\n",
      "       contains(imagina) = True                1 : 3      =     14.4 : 1.0\n",
      "            contains(tl) = True                1 : 3      =     14.1 : 1.0\n",
      "       contains(tadinha) = True                3 : 1      =     13.9 : 1.0\n",
      "           contains(dÃ³i) = True                3 : 1      =     13.3 : 1.0\n",
      "          contains(lima) = True                3 : 1      =     13.1 : 1.0\n",
      "         contains(saÃºde) = True                1 : 3      =     12.6 : 1.0\n",
      "            contains(vÃ³) = True                3 : 1      =     12.2 : 1.0\n",
      "       contains(segunda) = True                3 : 1      =     12.2 : 1.0\n",
      "    contains(pernilongo) = True                1 : 3      =     12.2 : 1.0\n",
      "          contains(vira) = True                1 : 3      =     12.2 : 1.0\n",
      "           contains(tÃ£o) = True                1 : 3      =     12.0 : 1.0\n",
      "       contains(gustavo) = True                3 : 1      =     11.9 : 1.0\n",
      "          contains(povo) = True                1 : 3      =     11.9 : 1.0\n",
      "      contains(hospital) = True                3 : 1      =     11.3 : 1.0\n",
      "       contains(twitter) = True                1 : 3      =     11.3 : 1.0\n",
      "             contains(ðŸ˜­) = True                3 : 1      =     11.2 : 1.0\n",
      "         contains(sinto) = True                3 : 1      =     10.5 : 1.0\n",
      "           contains(rua) = True                1 : 3      =     10.2 : 1.0\n",
      "          contains(acho) = True                3 : 1      =      9.7 : 1.0\n",
      "          contains(aids) = True                1 : 3      =      9.7 : 1.0\n",
      "         contains(risco) = True                1 : 3      =      9.7 : 1.0\n",
      "       contains(cuidado) = True                1 : 3      =      9.6 : 1.0\n",
      "          contains(acha) = True                3 : 1      =      9.3 : 1.0\n",
      "        contains(denovo) = True                3 : 1      =      9.1 : 1.0\n",
      "      contains(acabando) = True                3 : 1      =      9.1 : 1.0\n",
      "      contains(madrinha) = True                3 : 1      =      9.1 : 1.0\n",
      "      contains(adivinha) = True                3 : 1      =      9.1 : 1.0\n",
      "      contains(epidemia) = True                1 : 3      =      8.9 : 1.0\n",
      "           contains(pai) = True                3 : 1      =      8.9 : 1.0\n",
      "       contains(repouso) = True                3 : 1      =      8.8 : 1.0\n",
      "     contains(resultado) = True                3 : 1      =      8.8 : 1.0\n",
      "          contains(mata) = True                1 : 3      =      8.8 : 1.0\n",
      "         contains(chega) = True                1 : 3      =      8.7 : 1.0\n",
      "           contains(mÃ£e) = True                3 : 1      =      8.5 : 1.0\n",
      "        contains(medico) = True                3 : 1      =      8.3 : 1.0\n",
      "             contains(ðŸ˜¢) = True                3 : 1      =      8.3 : 1.0\n",
      "         contains(levar) = True                3 : 1      =      8.0 : 1.0\n",
      "          contains(with) = True                3 : 1      =      7.7 : 1.0\n",
      "      contains(melhorei) = True                3 : 1      =      7.7 : 1.0\n",
      "             contains(ðŸ’Š) = True                3 : 1      =      7.7 : 1.0\n",
      "          contains(here) = True                3 : 1      =      7.7 : 1.0\n",
      "     contains(confirmar) = True                3 : 1      =      7.7 : 1.0\n",
      "          contains(devo) = True                3 : 1      =      7.7 : 1.0\n",
      "         contains(primo) = True                3 : 1      =      7.5 : 1.0\n",
      "           contains(mae) = True                3 : 1      =      7.4 : 1.0\n",
      "         contains(surto) = True                1 : 3      =      7.1 : 1.0\n",
      "      contains(suspeita) = True                3 : 1      =      7.0 : 1.0\n",
      "           contains(fim) = True                1 : 3      =      6.9 : 1.0\n",
      "         contains(banho) = True                1 : 3      =      6.8 : 1.0\n",
      "           contains(dar) = True                1 : 3      =      6.7 : 1.0\n",
      "             contains(ðŸ˜”) = True                3 : 1      =      6.7 : 1.0\n",
      "          contains(anda) = True                1 : 3      =      6.5 : 1.0\n",
      "             contains(ðŸ˜¬) = True                3 : 1      =      6.3 : 1.0\n",
      "         contains(viado) = True                3 : 1      =      6.3 : 1.0\n",
      "       contains(familia) = True                3 : 1      =      6.3 : 1.0\n",
      "         contains(possa) = True                3 : 1      =      6.3 : 1.0\n",
      "      contains(cuidando) = True                3 : 1      =      6.3 : 1.0\n",
      "        contains(amanha) = True                3 : 1      =      6.3 : 1.0\n",
      "         contains(acham) = True                3 : 1      =      6.3 : 1.0\n",
      "          contains(miga) = True                3 : 1      =      6.3 : 1.0\n",
      "             contains(t) = True                3 : 1      =      6.3 : 1.0\n",
      "    contains(assistindo) = True                3 : 1      =      6.3 : 1.0\n",
      "        contains(chorar) = True                3 : 1      =      6.3 : 1.0\n",
      "      contains(namorado) = True                3 : 1      =      6.3 : 1.0\n",
      "     contains(internado) = True                3 : 1      =      6.3 : 1.0\n",
      "          contains(orem) = True                3 : 1      =      6.3 : 1.0\n",
      "        contains(cÃ³lica) = True                3 : 1      =      6.3 : 1.0\n",
      "             contains(ðŸ˜ž) = True                3 : 1      =      6.3 : 1.0\n",
      "             contains(ðŸ˜¥) = True                3 : 1      =      6.3 : 1.0\n",
      "        contains(rÃ¡pido) = True                1 : 3      =      6.3 : 1.0\n",
      "            contains(dÃ¡) = True                1 : 3      =      6.2 : 1.0\n",
      "        contains(exames) = True                3 : 1      =      6.0 : 1.0\n",
      "      contains(sintomas) = True                3 : 1      =      6.0 : 1.0\n",
      "      contains(qualquer) = True                1 : 3      =      5.9 : 1.0\n",
      "        contains(existe) = True                1 : 3      =      5.9 : 1.0\n",
      "          contains(toma) = True                1 : 3      =      5.9 : 1.0\n",
      "         contains(sobre) = True                1 : 3      =      5.8 : 1.0\n",
      "          contains(prof) = True                3 : 1      =      5.7 : 1.0\n",
      "        contains(beleza) = True                3 : 1      =      5.7 : 1.0\n",
      "             contains(ðŸ˜«) = True                3 : 1      =      5.7 : 1.0\n",
      "       contains(consigo) = True                3 : 1      =      5.7 : 1.0\n",
      "      contains(possÃ­vel) = True                3 : 1      =      5.7 : 1.0\n",
      "       contains(tadinho) = True                3 : 1      =      5.6 : 1.0\n",
      "      contains(morrendo) = True                3 : 1      =      5.6 : 1.0\n",
      "       contains(faltava) = True                3 : 1      =      5.6 : 1.0\n",
      "           contains(dor) = True                3 : 1      =      5.5 : 1.0\n",
      "         contains(passa) = True                1 : 3      =      5.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print informative features about the classifier\n",
    "print(NBClassifier.show_most_informative_features(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write tweets into .csv files for \n",
    "# data2 = []\n",
    "# with open('tweet_master_data.json') as f:\n",
    "#     for line in f:\n",
    "#         data2.extend(json.loads(line))\n",
    "\n",
    "import csv\n",
    "fp = open('brazil_sickness_tweets_val_NB.csv', 'w', newline='')\n",
    "a = csv.writer(fp, delimiter=',')\n",
    "a.writerow(('Time Stamp', 'Tweet Longitude', 'Tweet Latitude', 'Country', 'Region', 'State', 'County', 'Microregion'))\n",
    "\n",
    "# Writing of commonly annotated sickness tweets into the csv file\n",
    "for k in range(0,len(cmntags)):\n",
    "    tweet = cmntags[k]\n",
    "    cid = tweet['edits'][0]['curator_id']\n",
    "    tag = tweet['tags'][cid]\n",
    "    if(tag == 3):\n",
    "        a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classification of tweets annotated differently by different annotators\n",
    "for k in range(0,len(difftags)):\n",
    "    tweet = difftags[k]\n",
    "    if (tweet['lang'] != 'pt')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    testTweet = tweet['t']\n",
    "    fv = []\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    if (x == 3):\n",
    "        a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = []\n",
    "with open('tweet_master_data2.json') as f:\n",
    "    for line in f:\n",
    "        data2.extend(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification of non-validated tweets\n",
    "for k in range(0,len(data2)):\n",
    "    tweet = data2[k]\n",
    "    if (tweet['lang'] != 'pt')|(tweet['country'] != 'Brasil'):\n",
    "        continue\n",
    "    testTweet = tweet['t']\n",
    "    fv = []\n",
    "    processedTestTweet = processTweet(testTweet)\n",
    "    x = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet,fv)))\n",
    "    if (x == 3):\n",
    "        a.writerow((tweet['cr'], tweet['tln'], tweet['tlt'], tweet['country'], tweet['region'], tweet['state'], tweet['county'], tweet['microregion']))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
